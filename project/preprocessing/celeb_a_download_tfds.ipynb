{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6e0b34-63d0-4ad4-990b-e310d0d98efc",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e09ab-53c1-400e-bb06-6d5da500bd69",
   "metadata": {},
   "source": [
    "Notebook para fazer download de datasets de TFDS (Tensorflow Datasets).\n",
    "\n",
    "Datasets baixados:\n",
    "* Celeb A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ed2f3-9494-4293-ab51-ada523fa6186",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8cb280-2639-4257-b239-1698ad020c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 21:46:45.480474: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import shutil\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc2bf1-4204-46af-a0ad-8f2dd3f207f4",
   "metadata": {},
   "source": [
    "# Datasets Root Directory Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e58b2b-9ed2-4efc-b3d9-afef30955554",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_ROOT_DIR = '/home/guilherme/data1/Dropbox/Link to Desktop/Doutorado/Datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c422e4-db69-44a0-af1f-0595755b0f85",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a8f7b6-aa9f-44eb-850e-1ad5f686c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_celeba_split(ds):\n",
    "    df = tfds.as_dataframe(ds)\n",
    "    df_labels = pd.get_dummies(df.label)\n",
    "    df = pd.concat([df, df_labels], axis=1)\n",
    "    #df = df.drop(columns=['label'], inplace=False)\n",
    "    #df = df.rename(columns={x: f'n_{x}' for x in df.columns if type(x) is int}, inplace=False)\n",
    "    #df['img_name'] = [f'image_{x}' for x in range(ds.cardinality())]\n",
    "    return df\n",
    "\n",
    "\n",
    "def record_dataset(df, ds_split, split_name, ds_name):\n",
    "    dir_path = os.path.join(f'{DATASETS_ROOT_DIR}', ds_name, split_name)\n",
    "    shutil.rmtree(dir_path, ignore_errors=True)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    for idx,row in df.iterrows():        \n",
    "        img_path = os.path.join(dir_path, row.img_name + '.jpg')\n",
    "        cv2.imwrite(img_path, row.image)\n",
    "        df.loc[idx, 'img_name'] = img_path\n",
    "    \n",
    "    split_labels_df = df[['img_name']+[f'n_{x}' for x in range(10)]]\n",
    "    \n",
    "    labels_dir_path = os.path.join(DATASETS_ROOT_DIR, ds_name)\n",
    "    os.makedirs(labels_dir_path, exist_ok=True)\n",
    "    \n",
    "    labels_file_path = os.path.join(labels_dir_path, split_name + '_data.csv')\n",
    "    split_labels_df.to_csv(labels_file_path, index=False)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229c6f2-117c-43b3-8f7a-e8a3b0f67060",
   "metadata": {},
   "source": [
    "# Celeb A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544c016c-be21-410c-bb46-22c9be57a6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The version of the dataset you are trying to use (celeb_a/2.0.0) is too old for this version of TFDS so cannot be generated.Either sync to a previous version of TFDS to first prepare the data or use another version of the dataset. Available for `download_and_prepare`: ['2.0.1']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_128398/3096480458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#celeba_buider.supported_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'celeb_a:2.0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#df_train = format_celeba_split(ds_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mteval-icao-reqs/submodules/icao_nets_training/lib/python3.8/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mteval-icao-reqs/submodules/icao_nets_training/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    391\u001b[0m       msg += \"Available for `download_and_prepare`: {}\".format(\n\u001b[1;32m    392\u001b[0m           list(sorted(installable_versions)))\n\u001b[0;32m--> 393\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;31m# Currently it's not possible to overwrite the data because it would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The version of the dataset you are trying to use (celeb_a/2.0.0) is too old for this version of TFDS so cannot be generated.Either sync to a previous version of TFDS to first prepare the data or use another version of the dataset. Available for `download_and_prepare`: ['2.0.1']"
     ]
    }
   ],
   "source": [
    "#[ds_train, ds_valid, ds_test]\n",
    "#celeba_buider = tfds.builder('celeb_a', try_gcs=True) #load('celeb_a', split=['train','validation','test'], shuffle_files=True)\n",
    "#celeba_buider.supported_versions\n",
    "\n",
    "ds, df = tfds.load('celeb_a:2.0.0', shuffle_files=True)\n",
    "\n",
    "#df_train = format_celeba_split(ds_train)\n",
    "#df_valid = format_celeba_split(ds_valid)\n",
    "#df_test = format_celeba_split(ds_test)\n",
    "  \n",
    "#display(df_train.head())\n",
    "#display(df_valid.head())\n",
    "#display(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f42daa-cc02-495f-82b0-59f1e2f01040",
   "metadata": {},
   "source": [
    "## Record Images Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007139ef-b37f-4060-af35-794505f767f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_dataset(df_train, ds_train, 'train', 'fashion_mnist')\n",
    "record_dataset(df_valid, ds_valid, 'valid', 'fashion_mnist')\n",
    "record_dataset(df_test, ds_test, 'test', 'fashion_mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
