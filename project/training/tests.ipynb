{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebcb5fb4-4014-498c-8156-e0287fd84d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ('T1', 'T2'),\n",
       " 2: ('T1', 'T3'),\n",
       " 3: ('T1', 'T4'),\n",
       " 4: ('T2', 'T3'),\n",
       " 5: ('T2', 'T4'),\n",
       " 6: ('T3', 'T4'),\n",
       " 7: ('T1', 'T2', 'T3'),\n",
       " 8: ('T1', 'T2', 'T4'),\n",
       " 9: ('T1', 'T3', 'T4'),\n",
       " 10: ('T2', 'T3', 'T4'),\n",
       " 11: ('T1', 'T2', 'T3', 'T4')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def subsets(nums):\n",
    "    result = []\n",
    "    for i in range(len(nums) + 1):\n",
    "        result += itertools.combinations(nums, i)\n",
    "    return result\n",
    "\n",
    "MIN_TASK_GROUP_SIZE = 2\n",
    "\n",
    "def get_vocab_task_groups():\n",
    "    tasks = ['T1','T2','T3','T4']\n",
    "    tasks_groups_list = subsets(tasks)\n",
    "    tasks_groups_list = [x for x in tasks_groups_list if len(x) >= MIN_TASK_GROUP_SIZE]\n",
    "    \n",
    "    tasks_groups_params = []\n",
    "    tasks_groups_id = []\n",
    "    \n",
    "    for j in range(len(tasks_groups_list)):\n",
    "        tasks_groups_params.append((tasks_groups_list[j]))\n",
    "        tasks_groups_id.append(j+1)\n",
    "    \n",
    "    vocab = dict(zip(tasks_groups_id, tasks_groups_params))\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "get_vocab_task_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d42ecd-0662-4faa-8caa-ea4e037dbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_architecture_sequences(self, model, number_of_samples):\n",
    "    final_layer_id = len(self.vocab)\n",
    "    dropout_id = final_layer_id - 1\n",
    "    vocab_idx = [0] + list(self.vocab.keys())\n",
    "    samples = []\n",
    "    print(\"GENERATING ARCHITECTURE SAMPLES...\")\n",
    "    print('------------------------------------------------------')\n",
    "    while len(samples) < number_of_samples:\n",
    "        seed = []\n",
    "        while len(seed) < self.max_len:\n",
    "            sequence = pad_sequences([seed], maxlen=self.max_len - 1, padding='post')\n",
    "            sequence = sequence.reshape(1, 1, self.max_len - 1)\n",
    "            if self.use_predictor:\n",
    "                (probab, _) = model.predict(sequence)\n",
    "            else:\n",
    "                probab = model.predict(sequence)\n",
    "            probab = probab[0][0]\n",
    "            next = np.random.choice(vocab_idx, size=1, p=probab)[0]\n",
    "            if next == dropout_id and len(seed) == 0:\n",
    "                continue\n",
    "            if next == final_layer_id and len(seed) == 0:\n",
    "                continue\n",
    "            if next == final_layer_id:\n",
    "                seed.append(next)\n",
    "                break\n",
    "            if len(seed) == self.max_len - 1:\n",
    "                seed.append(final_layer_id)\n",
    "                break\n",
    "            if not next == 0:\n",
    "                seed.append(next)\n",
    "\n",
    "        if self.__check_sequence_validity(seed):\n",
    "            samples.append(seed)\n",
    "            self.seq_data.append(seed)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8ea76-6f1a-43a9-8ba0-00c78b115401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb65ef6-54ba-4b52-86ee-a6cb6d3e8a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ('n_denses_0', 1),\n",
       " 2: ('n_denses_1', 1),\n",
       " 3: ('n_denses_2', 1),\n",
       " 4: ('n_denses_3', 1),\n",
       " 5: ('n_denses_0', 2),\n",
       " 6: ('n_denses_1', 2),\n",
       " 7: ('n_denses_2', 2),\n",
       " 8: ('n_denses_3', 2),\n",
       " 9: ('n_denses_0', 3),\n",
       " 10: ('n_denses_1', 3),\n",
       " 11: ('n_denses_2', 3),\n",
       " 12: ('n_denses_3', 3),\n",
       " 13: ('n_denses_0', 4),\n",
       " 14: ('n_denses_1', 4),\n",
       " 15: ('n_denses_2', 4),\n",
       " 16: ('n_denses_3', 4),\n",
       " 17: ('n_denses_0', 5),\n",
       " 18: ('n_denses_1', 5),\n",
       " 19: ('n_denses_2', 5),\n",
       " 20: ('n_denses_3', 5)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_vocab_orig():\n",
    "    list_n_fcs = [1,2,3,4,5]\n",
    "    layers = ['n_denses_0','n_denses_1','n_denses_2','n_denses_3']\n",
    "    layers_params = []\n",
    "    layer_id = []\n",
    "    for i in range(len(list_n_fcs)):\n",
    "        for j in range(len(layers)):\n",
    "            layers_params.append((layers[j], list_n_fcs[i]))\n",
    "            layer_id.append(len(layers) * i + j + 1)\n",
    "    vocab = dict(zip(layer_id, layers_params))\n",
    "    return vocab\n",
    "\n",
    "get_vocab_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7131b4-42f7-4e93-9046-b457e52777d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2979d-9ef8-44b9-877d-eed1ca4e837f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449508c-6991-483c-ac8b-33189abe8913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff00fba-8b7b-47e2-90db-b99319557746",
   "metadata": {},
   "source": [
    "# Testing Network Modification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c789cdb-97b9-43ba-b201-b20b08f00ce1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_tensor = Input(shape=(20,), name=\"input\")\n",
    "hidden = Dense(100, activation='relu')(input_tensor)\n",
    "out = Dense(10, activation='relu', name=\"out\")(hidden)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=out)\n",
    "model.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "out = Dense(5, activation='softmax', name='new_out')(model.layers[-2].output)\n",
    "\n",
    "new_model = Model(input_tensor, out)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f294d-dd93-430f-8d4b-4e3ae8e9bb40",
   "metadata": {},
   "source": [
    "# Test - Customized Loss Function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15e6d1ce-12d5-4e1e-b41e-aab37d59c089",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "def my_loss_func(y_true, y_pred):\n",
    "    print(y_true, y_pred)\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "\n",
    "def _controller_loss(y_true, y_pred):\n",
    "    baseline = None\n",
    "    baseline_decay = 0.999\n",
    "    reward = 0\n",
    "\n",
    "    if baseline is None:\n",
    "        baseline = 0\n",
    "    else:\n",
    "        baseline -= (1 - baseline_decay) * (baseline - reward)\n",
    "    return y_pred * (reward - baseline)\n",
    "\n",
    "def _define_loss(controller_loss):\n",
    "    print(controller_loss)\n",
    "    a =  {f\"out\": controller_loss for i in range(4)}\n",
    "    print(a)\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=(4), name=\"input\")\n",
    "hidden = Dense(64, activation='relu')(input_tensor)\n",
    "out = Dense(4, activation='softmax', name=\"out\")(hidden)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=out)\n",
    "#model.compile(loss=my_loss_func, optimizer='adam')\n",
    "model.compile(loss=_define_loss(_controller_loss), optimizer='adam')\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "#x = np.random.rand(4,4)\n",
    "#y = np.random.rand(4,1)\n",
    "\n",
    "x = np.matrix([[2,1,4,5], [3,4,5,2]], dtype='float32').A\n",
    "y = np.array([4,2], dtype='float32')\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "H = model.fit(x, y, epochs=3, batch_size=1)\n",
    "H.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fca27-778a-46ce-802c-bcdf6c695d43",
   "metadata": {},
   "source": [
    "# Test - "
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa7ea0d3-b1bb-478b-a186-85d57e8e0e24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "# disable tensorflow log level infos\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # show only errors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def __create_rnn_model():\n",
    "    model = Sequential([\n",
    "        Dense(4, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(4, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def __preprocess_config(config):\n",
    "    return np.linalg.norm(config)\n",
    "    \n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "X = np.random.rand(400,4)\n",
    "y = np.random.rand(400,4)\n",
    "\n",
    "# X = tf.expand_dims(X, axis=0)\n",
    "# y = np.expand_dims(y, axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_test = np.random.rand(20,4)\n",
    "y_test = np.random.rand(20,4)\n",
    "\n",
    "# X_test = tf.expand_dims(X_test, axis=0)\n",
    "# y_test = tf.expand_dims(y_test, axis=0)\n",
    "\n",
    "m = __create_rnn_model()\n",
    "\n",
    "m.fit(X,y, batch_size=32, epochs=5)\n",
    "\n",
    "loss, acc = m.evaluate(X_test,y_test, batch_size=32)\n",
    "\n",
    "print(f'loss: {loss}%')\n",
    "print(f'acc: {round(acc*100,2)}%')\n",
    "\n",
    "print(f'prediction: {m.predict(np.array(X_test[0]).reshape(1,4))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
