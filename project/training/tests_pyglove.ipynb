{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c69f7f-444c-49ec-9348-a7e0dd942259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAS on MNIST with compositional symbolic objects.\\n\\nThis is a basic working ML program which does NAS on MNIST.\\nThe code is modified from the tf.keras tutorial here:\\nhttps://www.tensorflow.org/tutorials/keras/classification\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright 2019 The PyGlove Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"NAS on MNIST with compositional symbolic objects.\n",
    "\n",
    "This is a basic working ML program which does NAS on MNIST.\n",
    "The code is modified from the tf.keras tutorial here:\n",
    "https://www.tensorflow.org/tutorials/keras/classification\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86f534c-8894-4e9a-98ac-3ac4d1e38cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 21:17:09.110386: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import pyglove as pg\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#flags.DEFINE_integer('max_trials', 10, 'Number of max trials for tuning.')\n",
    "#flags.DEFINE_integer('num_epochs', 10, 'Number of epochs to train for each trail.')\n",
    "\n",
    "# Placeholder for Google-internal tuning backend flags.\n",
    "\n",
    "\n",
    "#FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a947cd7-1d4e-4732-8c10-89be7b49eef3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ==> Restrict GPU memory growth: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 21:17:20.831338: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-03-14 21:17:20.890407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-14 21:17:20.891122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:06:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.785GHz coreCount: 40 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2023-03-14 21:17:20.891163: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-03-14 21:17:20.916186: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-03-14 21:17:20.916261: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-03-14 21:17:20.926381: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-14 21:17:20.930989: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-14 21:17:20.936389: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-03-14 21:17:20.941841: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-03-14 21:17:20.942992: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-03-14 21:17:20.943139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-14 21:17:20.943852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-14 21:17:20.950984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "# disable tensorflow log level infos\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # show only errors\n",
    "\n",
    "\n",
    "## restrict memory growth -------------------\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try:\n",
    "    gpu_0 = physical_devices[0]\n",
    "    tf.config.experimental.set_memory_growth(gpu_0, True) \n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu_0, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6500)])\n",
    "    print(' ==> Restrict GPU memory growth: True')\n",
    "except: \n",
    "    raise Exception(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
    "## restrict memory growth ------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df74f6b6-257f-4064-8af1-f60823eefe59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: DNA: DNA(0, [0, 1])\n",
      "Sequential(\n",
      "  layers = [\n",
      "    0 : <tensorflow.python.keras.layers.core.Flatten object at 0x7fb2c2fc6160>,\n",
      "    1 : Dense(\n",
      "      units = 64,\n",
      "      activation = 'sigmoid',\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    2 : <tensorflow.python.keras.layers.core.Dense object at 0x7fb2bc5aebb0>\n",
      "  ],\n",
      "  name = None\n",
      ")\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 2s 995us/step - loss: 0.4763 - accuracy: 0.8837\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2216 - accuracy: 0.9367\n",
      "313/313 - 0s - loss: 0.1865 - accuracy: 0.9472\n",
      "2: DNA: DNA(0, [0, 1])\n",
      "Sequential(\n",
      "  layers = [\n",
      "    0 : <tensorflow.python.keras.layers.core.Flatten object at 0x7fb2bbd0beb0>,\n",
      "    1 : Dense(\n",
      "      units = 64,\n",
      "      activation = 'sigmoid',\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    2 : <tensorflow.python.keras.layers.core.Dense object at 0x7fb2bbd0c550>\n",
      "  ],\n",
      "  name = None\n",
      ")\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4592 - accuracy: 0.8882\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2195 - accuracy: 0.9377\n",
      "313/313 - 0s - loss: 0.1873 - accuracy: 0.9456\n",
      "3: DNA: DNA(1, [0, 0, 0])\n",
      "Sequential(\n",
      "  layers = [\n",
      "    0 : <tensorflow.python.keras.layers.core.Lambda object at 0x7fb2ced4fc40>,\n",
      "    1 : Conv2D(\n",
      "      filters = 64,\n",
      "      kernel_size = (3, 3),\n",
      "      strides = (1, 1),\n",
      "      padding = 'same',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = 'relu',\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    2 : <tensorflow.python.keras.layers.core.Flatten object at 0x7fb2ced4fee0>,\n",
      "    3 : <tensorflow.python.keras.layers.core.Dense object at 0x7fb2ced3d490>\n",
      "  ],\n",
      "  name = None\n",
      ")\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1538 - accuracy: 0.9562\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0604 - accuracy: 0.9820\n",
      "313/313 - 0s - loss: 0.0624 - accuracy: 0.9802\n",
      "Top 10 results.\n",
      "# 1 - trial  3 (0.980): DNA(1, [0, 0, 0])\n",
      "# 2 - trial  1 (0.947): DNA(0, [0, 1])\n",
      "# 3 - trial  2 (0.946): DNA(0, [0, 1])\n"
     ]
    }
   ],
   "source": [
    "def download_and_prep_data() -> Tuple[np.ndarray,np.ndarray,np.ndarray,np.ndarray]:\n",
    "    \"\"\"Download dataset and scale to [0, 1].\n",
    "\n",
    "      Returns:\n",
    "        tr_x: Training data.\n",
    "        tr_y: Training labels.\n",
    "        te_x: Testing data.\n",
    "        te_y: Testing labels.\n",
    "    \"\"\"\n",
    "    mnist_dataset = tf.keras.datasets.mnist\n",
    "    (tr_x, tr_y), (te_x, te_y) = mnist_dataset.load_data()\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "    return tr_x, tr_y, te_x, te_y\n",
    "\n",
    "\n",
    "# NOTE(daiyip): We symbolize three Keras layers, so that their hyper-parameters\n",
    "# can accept hyper values such as `pg.oneof`. Therefore, a search space for the\n",
    "# model architecture can be represented as a hyper Sequential object.\n",
    "Conv2D = pg.symbolize(tf.keras.layers.Conv2D)\n",
    "Dense = pg.symbolize(tf.keras.layers.Dense)\n",
    "Sequential = pg.symbolize(tf.keras.Sequential)\n",
    "\n",
    "\n",
    "def nas_model():\n",
    "    \"\"\"NAS search space.\"\"\"\n",
    "    return Sequential(layers=pg.oneof([\n",
    "          # Model family 1: only dense layers.\n",
    "          [\n",
    "              tf.keras.layers.Flatten(),\n",
    "              # NOTE(daiyip): we use the symbolic Dense here as `pg.oneof` are\n",
    "              # passed to its constructor to create a search space on Dense\n",
    "              # hyper-parameters. On the next line, we use the regular Keras\n",
    "              # Dense class since we don't tune its hyper-parameters, though\n",
    "              # the symbolic Dense can also work on fixed hyper-parameter values.\n",
    "              Dense(pg.oneof([64, 128]), activation=pg.oneof(['relu', 'sigmoid'])),\n",
    "              tf.keras.layers.Dense(10, activation='softmax')\n",
    "          ],\n",
    "          # Model family 2: conv net.\n",
    "          [\n",
    "              tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 28, 28, 1))),\n",
    "              Conv2D(filters=pg.oneof([64, 128]),\n",
    "                     kernel_size=pg.oneof([(3, 3), (5, 5)]),\n",
    "                     padding='same',\n",
    "                     activation=pg.oneof(['relu', 'sigmoid'])),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(10, activation='softmax')\n",
    "          ]\n",
    "    ]))\n",
    "\n",
    "\n",
    "def train_and_eval(model, input_data, num_epochs=10) -> float:\n",
    "    \"\"\"Returns model accuracy after train and evaluation.\"\"\"\n",
    "    tr_x, tr_y, te_x, te_y = input_data\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    model.fit(tr_x, tr_y, epochs=num_epochs)\n",
    "    _, test_acc = model.evaluate(te_x, te_y, verbose=2)\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "class MyDNAGenerator(pg.DNAGenerator):\n",
    "    def _propose(self):\n",
    "        return pg.DNA()[1,1]\n",
    "\n",
    "\n",
    "def tune(max_trials, num_epochs):\n",
    "    \"\"\"Tune MNIST model via random search.\"\"\"\n",
    "    results = []\n",
    "    input_data = download_and_prep_data()\n",
    "    # NOTE(daiyip): `pg.sample` returns an iterator of (example, feedback_fn)\n",
    "    # from a hyper object (the search space) and a DNAGenerator (the search\n",
    "    # algorithm), with an optional flag to set the max examples to sample.\n",
    "    # `example` is a materialized object of the search space, and `feedback_fn`\n",
    "    # is a callable object that we can send back a float reward to the\n",
    "    # controller. `feedback_fn` also has a property `dna` to access the DNA value\n",
    "    # of current example.\n",
    "    \n",
    "    #my_gen = MyDNAGenerator()\n",
    "    \n",
    "    \n",
    "    for model, feedback in pg.sample(nas_model(), pg.generators.Random(), max_trials):\n",
    "        print('{}: DNA: {}'.format(feedback.id, feedback.dna))\n",
    "        print(model)\n",
    "        test_acc = train_and_eval(model, input_data, num_epochs)\n",
    "        results.append((feedback.id, feedback.dna, test_acc))\n",
    "        # NOTE: for random generator, following call to `feedback` is a no-op.\n",
    "        # We keep it here in case we want to change algorithm.\n",
    "        feedback(test_acc)\n",
    "  \n",
    "    # Print best results.\n",
    "    top_results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    print('Top 10 results.')\n",
    "    for i, (trial_id, dna, test_acc) in enumerate(top_results[:10]):\n",
    "        print('#{0:2d} - trial {1:2d} ({2:.3f}): {3}'.format(i + 1, trial_id, test_acc, dna))\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    if len(argv) > 1:\n",
    "        raise app.UsageError('Too many command-line arguments.')\n",
    "\n",
    "    # Placeholder for Google-internal tuning backend setup.\n",
    "\n",
    "    tune(FLAGS.max_trials, FLAGS.num_epochs)\n",
    "\n",
    "\n",
    "tune(3, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
