{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fd12f9-43c2-4cd0-b91c-31a3fef7844e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cc6625-c6b7-4482-a75e-13b2cfc4cedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# disable tensorflow log level infos\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # show only errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86f534c-8894-4e9a-98ac-3ac4d1e38cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import pyglove as pg\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b783cf2-4b9b-4baa-8d9e-a5d725ba0f6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a947cd7-1d4e-4732-8c10-89be7b49eef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ==> Restrict GPU memory growth: True\n"
     ]
    }
   ],
   "source": [
    "## restrict memory growth -------------------\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try:\n",
    "    gpu_0 = physical_devices[0]\n",
    "    tf.config.experimental.set_memory_growth(gpu_0, True) \n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu_0, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6500)])\n",
    "    print(' ==> Restrict GPU memory growth: True')\n",
    "except: \n",
    "    raise Exception(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
    "## restrict memory growth ------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4daafd3-364c-488b-90f2-4d5a4f6d0d67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Utilitary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275084d2-c9e4-421f-a108-b8c4b936f8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_prep_data() -> Tuple[np.ndarray,np.ndarray,np.ndarray,np.ndarray]:\n",
    "    \"\"\"Download dataset and scale to [0, 1].\n",
    "\n",
    "      Returns:\n",
    "        tr_x: Training data.\n",
    "        tr_y: Training labels.\n",
    "        te_x: Testing data.\n",
    "        te_y: Testing labels.\n",
    "    \"\"\"\n",
    "    mnist_dataset = tf.keras.datasets.mnist\n",
    "    (tr_x, tr_y), (te_x, te_y) = mnist_dataset.load_data()\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "    return tr_x, tr_y, te_x, te_y\n",
    "\n",
    "\n",
    "\n",
    "def train_and_eval(model, input_data, num_epochs=10) -> float:\n",
    "    \"\"\"Returns model accuracy after train and evaluation.\"\"\"\n",
    "    tr_x, tr_y, te_x, te_y = input_data\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    model.fit(tr_x, tr_y, epochs=num_epochs)\n",
    "    _, test_acc = model.evaluate(te_x, te_y, verbose=2)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f1c3d-2308-44fc-ae63-054ac7381baf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Symbolize Keras Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b52ecf6-b9fa-4866-bd23-e463fbea6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE(daiyip): We symbolize three Keras layers, so that their hyper-parameters\n",
    "# can accept hyper values such as `pg.oneof`. Therefore, a search space for the\n",
    "# model architecture can be represented as a hyper Sequential object.\n",
    "Conv2D = pg.symbolize(tf.keras.layers.Conv2D)\n",
    "Dense = pg.symbolize(tf.keras.layers.Dense)\n",
    "MaxPool2D = pg.symbolize(tf.keras.layers.MaxPool2D)\n",
    "Identity = pg.symbolize(tf.keras.initializers.Identity)\n",
    "Sequential = pg.symbolize(tf.keras.Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249abad-6445-485c-87e6-18ef003cd24d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df74f6b6-257f-4064-8af1-f60823eefe59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nas_model():\n",
    "    \"\"\"NAS search space.\"\"\"\n",
    "    return Sequential(layers=pg.oneof([\n",
    "          # Model family 1: only dense layers.\n",
    "          [\n",
    "              tf.keras.layers.Flatten(),\n",
    "              # NOTE(daiyip): we use the symbolic Dense here as `pg.oneof` are\n",
    "              # passed to its constructor to create a search space on Dense\n",
    "              # hyper-parameters. On the next line, we use the regular Keras\n",
    "              # Dense class since we don't tune its hyper-parameters, though\n",
    "              # the symbolic Dense can also work on fixed hyper-parameter values.\n",
    "              Dense(pg.oneof([64, 128]), activation=pg.oneof(['relu', 'sigmoid'])),\n",
    "              tf.keras.layers.Dense(10, activation='softmax')\n",
    "          ],\n",
    "          # Model family 2: conv net.\n",
    "          [\n",
    "              tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 28, 28, 1))),\n",
    "              Conv2D(filters=pg.oneof([64, 128]),\n",
    "                     kernel_size=pg.oneof([(3, 3), (5, 5)]),\n",
    "                     padding='same',\n",
    "                     activation=pg.oneof(['relu', 'sigmoid'])),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(10, activation='softmax')\n",
    "          ]\n",
    "    ]))\n",
    "\n",
    "\n",
    "\n",
    "# class MyDNAGenerator(pg.DNAGenerator):\n",
    "#     def _propose(self):\n",
    "#         return pg.DNA()[1,1]\n",
    "\n",
    "\n",
    "def tune(max_trials, num_epochs):\n",
    "    \"\"\"Tune MNIST model via random search.\"\"\"\n",
    "    results = []\n",
    "    input_data = download_and_prep_data()\n",
    "    # NOTE(daiyip): `pg.sample` returns an iterator of (example, feedback_fn)\n",
    "    # from a hyper object (the search space) and a DNAGenerator (the search\n",
    "    # algorithm), with an optional flag to set the max examples to sample.\n",
    "    # `example` is a materialized object of the search space, and `feedback_fn`\n",
    "    # is a callable object that we can send back a float reward to the\n",
    "    # controller. `feedback_fn` also has a property `dna` to access the DNA value\n",
    "    # of current example.\n",
    "    \n",
    "    #my_gen = MyDNAGenerator()\n",
    "    \n",
    "    for model, feedback in pg.sample(nas_model(), pg.generators.Random(), max_trials):\n",
    "        print('{}: DNA: {}'.format(feedback.id, feedback.dna))\n",
    "        print(model)\n",
    "        test_acc = train_and_eval(model, input_data, num_epochs)\n",
    "        results.append((feedback.id, feedback.dna, test_acc))\n",
    "        # NOTE: for random generator, following call to `feedback` is a no-op.\n",
    "        # We keep it here in case we want to change algorithm.\n",
    "        feedback(test_acc)\n",
    "  \n",
    "    # Print best results.\n",
    "    top_results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    print('Top 10 results.')\n",
    "    for i, (trial_id, dna, test_acc) in enumerate(top_results[:10]):\n",
    "        print('#{0:2d} - trial {1:2d} ({2:.3f}): {3}'.format(i + 1, trial_id, test_acc, dna))\n",
    "\n",
    "\n",
    "tune(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d11dd-e05a-4852-9c50-77bd64176c8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5008e-4551-4643-bd7a-96dbaf861525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nas_model():\n",
    "    \"\"\"NAS search space.\"\"\"\n",
    "    return Sequential(layers=pg.oneof([\n",
    "          # Model family 1: only dense layers.\n",
    "          [\n",
    "              tf.keras.layers.Flatten(),\n",
    "              # NOTE(daiyip): we use the symbolic Dense here as `pg.oneof` are\n",
    "              # passed to its constructor to create a search space on Dense\n",
    "              # hyper-parameters. On the next line, we use the regular Keras\n",
    "              # Dense class since we don't tune its hyper-parameters, though\n",
    "              # the symbolic Dense can also work on fixed hyper-parameter values.\n",
    "              Dense(pg.oneof([64, 128]), activation=pg.oneof(['relu', 'sigmoid'])),\n",
    "              tf.keras.layers.Dense(10, activation='softmax')\n",
    "          ],\n",
    "          # Model family 2: conv net.\n",
    "          [\n",
    "              tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 28, 28, 1))),\n",
    "              Conv2D(filters=pg.oneof([64, 128]),\n",
    "                     kernel_size=pg.oneof([(3, 3), (5, 5)]),\n",
    "                     padding='same',\n",
    "                     activation=pg.oneof(['relu', 'sigmoid'])),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(10, activation='softmax')\n",
    "          ]\n",
    "    ]))\n",
    "\n",
    "\n",
    "def tune(max_trials, num_epochs):\n",
    "    \"\"\"Tune MNIST model via random search.\"\"\"\n",
    "    results = []\n",
    "    input_data = download_and_prep_data()\n",
    "    # NOTE(daiyip): `pg.sample` returns an iterator of (example, feedback_fn)\n",
    "    # from a hyper object (the search space) and a DNAGenerator (the search\n",
    "    # algorithm), with an optional flag to set the max examples to sample.\n",
    "    # `example` is a materialized object of the search space, and `feedback_fn`\n",
    "    # is a callable object that we can send back a float reward to the\n",
    "    # controller. `feedback_fn` also has a property `dna` to access the DNA value\n",
    "    # of current example.\n",
    "    \n",
    "    for model, feedback in pg.sample(nas_model(), pg.generators.Random(), max_trials):\n",
    "        print('{}: DNA: {}'.format(feedback.id, feedback.dna))\n",
    "        print(model)\n",
    "        test_acc = train_and_eval(model, input_data, num_epochs)\n",
    "        results.append((feedback.id, feedback.dna, test_acc))\n",
    "        # NOTE: for random generator, following call to `feedback` is a no-op.\n",
    "        # We keep it here in case we want to change algorithm.\n",
    "        feedback(test_acc)\n",
    "  \n",
    "    # Print best results.\n",
    "    top_results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    print('Top 10 results.')\n",
    "    for i, (trial_id, dna, test_acc) in enumerate(top_results[:10]):\n",
    "        print('#{0:2d} - trial {1:2d} ({2:.3f}): {3}'.format(i + 1, trial_id, test_acc, dna))\n",
    "\n",
    "\n",
    "tune(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbe73ab8-738b-42e1-a91e-9755166f96b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "@pg.symbolize\n",
    "class A:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "    \n",
    "    def _sum(self):\n",
    "        return self.a + self.b\n",
    "    \n",
    "\n",
    "_a = A(1,2)\n",
    "print(_a._sum())\n",
    "\n",
    "_a.rebind(a=5)\n",
    "\n",
    "print(_a._sum())\n",
    "\n",
    "_a.rebind(a=10, b=20)\n",
    "print(_a._sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55391bfd-4d2b-4114-871c-a997c52c0204",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring keyword arguments that are not supported by 'in-memory' backend: {'partition_fn': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1: DNA: DNA([(1, 0), 0])\n",
      "Trainer(\n",
      "  model = Stacked(\n",
      "    op = Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    repeats = 3\n",
      "  ),\n",
      "  optimizer = <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7efc7c4ba6d0>\n",
      ")\n",
      "Set op: Conv2D(\n",
      "  filters = 4,\n",
      "  kernel_size = (5, 5),\n",
      "  strides = (1, 1),\n",
      "  padding = 'valid',\n",
      "  data_format = None,\n",
      "  dilation_rate = (1, 1),\n",
      "  groups = 1,\n",
      "  activation = None,\n",
      "  use_bias = True,\n",
      "  kernel_initializer = 'glorot_uniform',\n",
      "  bias_initializer = 'zeros',\n",
      "  kernel_regularizer = None,\n",
      "  bias_regularizer = None,\n",
      "  activity_regularizer = None,\n",
      "  kernel_constraint = None,\n",
      "  bias_constraint = None\n",
      ") | Set repeats: 3\n",
      "self.optmizer: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7efc7c4ba6d0>\n",
      "self.model: Sequential(\n",
      "  layers = [\n",
      "    0 : <tensorflow.python.keras.layers.core.Lambda object at 0x7efc21b57310>,\n",
      "    1 : Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    2 : Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    3 : Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    4 : <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x7efc21b57040>,\n",
      "    5 : <tensorflow.python.keras.layers.core.Dense object at 0x7efc21b6a5b0>\n",
      "  ],\n",
      "  name = None\n",
      ")\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3353 - accuracy: 0.5270\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.1080 - accuracy: 0.6178\n",
      "313/313 - 0s - loss: 1.0297 - accuracy: 0.6425\n",
      "\n",
      "\n",
      "2: DNA: DNA([(1, 0), 1])\n",
      "Trainer(\n",
      "  model = Stacked(\n",
      "    op = Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    repeats = 3\n",
      "  ),\n",
      "  optimizer = <tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7efc7c4baaf0>\n",
      ")\n",
      "Set op: Conv2D(\n",
      "  filters = 4,\n",
      "  kernel_size = (5, 5),\n",
      "  strides = (1, 1),\n",
      "  padding = 'valid',\n",
      "  data_format = None,\n",
      "  dilation_rate = (1, 1),\n",
      "  groups = 1,\n",
      "  activation = None,\n",
      "  use_bias = True,\n",
      "  kernel_initializer = 'glorot_uniform',\n",
      "  bias_initializer = 'zeros',\n",
      "  kernel_regularizer = None,\n",
      "  bias_regularizer = None,\n",
      "  activity_regularizer = None,\n",
      "  kernel_constraint = None,\n",
      "  bias_constraint = None\n",
      ") | Set repeats: 3\n",
      "self.optmizer: <tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7efc7c4baaf0>\n",
      "self.model: Sequential(\n",
      "  layers = [\n",
      "    0 : <tensorflow.python.keras.layers.core.Lambda object at 0x7efc11a81ee0>,\n",
      "    1 : Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    2 : Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    3 : Conv2D(\n",
      "      filters = 4,\n",
      "      kernel_size = (5, 5),\n",
      "      strides = (1, 1),\n",
      "      padding = 'valid',\n",
      "      data_format = None,\n",
      "      dilation_rate = (1, 1),\n",
      "      groups = 1,\n",
      "      activation = None,\n",
      "      use_bias = True,\n",
      "      kernel_initializer = 'glorot_uniform',\n",
      "      bias_initializer = 'zeros',\n",
      "      kernel_regularizer = None,\n",
      "      bias_regularizer = None,\n",
      "      activity_regularizer = None,\n",
      "      kernel_constraint = None,\n",
      "      bias_constraint = None\n",
      "    ),\n",
      "    4 : <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x7efc21219fa0>,\n",
      "    5 : <tensorflow.python.keras.layers.core.Dense object at 0x7efc212419d0>\n",
      "  ],\n",
      "  name = None\n",
      ")\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:799 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:530 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:660 apply_gradients\n        apply_state = self._prepare(var_list)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:942 _prepare\n        self._prepare_local(var_device, var_dtype, apply_state)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/rmsprop.py:161 _prepare_local\n        super(RMSprop, self)._prepare_local(var_device, var_dtype, apply_state)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:948 _prepare_local\n        lr_t = array_ops.identity(self._decayed_lr(var_dtype))\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1002 _decayed_lr\n        lr_t = self._get_hyper(\"learning_rate\", var_dtype)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:799 _get_hyper\n        value = value()\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/pyglove/core/hyper/base.py:88 __call__\n        raise ValueError(\n\n    ValueError: 'set_dna' should be called to set a DNA before '__call__'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11530/1503915120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_11530/1503915120.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(max_trials, num_epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrebind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11530/1503915120.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data, num_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m                            metrics=['accuracy'])\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:799 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:530 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:660 apply_gradients\n        apply_state = self._prepare(var_list)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:942 _prepare\n        self._prepare_local(var_device, var_dtype, apply_state)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/rmsprop.py:161 _prepare_local\n        super(RMSprop, self)._prepare_local(var_device, var_dtype, apply_state)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:948 _prepare_local\n        lr_t = array_ops.identity(self._decayed_lr(var_dtype))\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1002 _decayed_lr\n        lr_t = self._get_hyper(\"learning_rate\", var_dtype)\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:799 _get_hyper\n        value = value()\n    /home/guilherme/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/pyglove/core/hyper/base.py:88 __call__\n        raise ValueError(\n\n    ValueError: 'set_dna' should be called to set a DNA before '__call__'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "@pg.symbolize\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "    \n",
    "    def train(self, input_data, num_epochs):    \n",
    "        tr_x, tr_y, te_x, te_y = input_data\n",
    "        \n",
    "        print(f'self.optmizer: {self.optimizer}')\n",
    "        print(f'self.model: {self.model}')\n",
    "        \n",
    "        self.model.compile(optimizer=self.optimizer, \n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        self.model.fit(tr_x, tr_y, epochs=num_epochs)\n",
    "        \n",
    "        _, test_acc = self.model.evaluate(te_x, te_y, verbose=2)\n",
    "        \n",
    "        return test_acc\n",
    "\n",
    "    \n",
    "    \n",
    "@pg.symbolize\n",
    "class Stacked(object):\n",
    "    def __init__(self, op, repeats):\n",
    "        self.op = op\n",
    "        self.repeats = repeats\n",
    "    \n",
    "    def __call__(self):\n",
    "        print(f'Set op: {self.op} | Set repeats: {self.repeats}')\n",
    "        return Sequential(layers=[tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 28, 28, 1)))]+\\\n",
    "                                 [self.op for _ in range(self.repeats)]+\\\n",
    "                                 [tf.keras.layers.GlobalAveragePooling2D()] +\n",
    "                                 [tf.keras.layers.Dense(10, activation='softmax')])  \n",
    "        \n",
    "\n",
    "\n",
    "def create_trainer():\n",
    "    #st = Stacked()\n",
    "    model = Stacked(op=pg.oneof([Conv2D(pg.oneof([4, 8]), (3, 3)), \n",
    "                                 Conv2D(pg.oneof([4, 8]), (5, 5))]), \n",
    "                    repeats=3)\n",
    "    return Trainer(model=model,\n",
    "                   optimizer=pg.oneof([Adam(2e-2), RMSprop(pg.floatv(1e-6, 1e-3))]))\n",
    "\n",
    "\n",
    "# def nas_model_2(repeats):\n",
    "#     layers = [tf.keras.layers.Flatten()] + \\\n",
    "#              [Dense(pg.oneof([64, 128]), \n",
    "#                     activation=pg.oneof(['relu', 'sigmoid']))] * repeats + \\\n",
    "#              [tf.keras.layers.Dense(10, activation='softmax')]\n",
    "#     return Sequential(layers=layers)\n",
    "\n",
    "\n",
    "def tune(max_trials, num_epochs):\n",
    "    \"\"\"Tune MNIST model via random search.\"\"\"\n",
    "    results = []\n",
    "    input_data = download_and_prep_data()\n",
    "        \n",
    "    for trainer, feedback in pg.sample(create_trainer(), pg.generators.Random(), max_trials, partition_fn=None):\n",
    "        print('\\n\\n{}: DNA: {}'.format(feedback.id, feedback.dna))\n",
    "        \n",
    "        #print(model)\n",
    "        #test_acc = train_and_eval(model, input_data, num_epochs)\n",
    "        \n",
    "        print(trainer)\n",
    "        \n",
    "        model_2 = trainer.model()\n",
    "        \n",
    "        #print(model_2)\n",
    "        \n",
    "        trainer.rebind(model=model_2)\n",
    "        \n",
    "        test_acc = trainer.train(input_data, num_epochs)\n",
    "        \n",
    "        results.append((feedback.id, feedback.dna, test_acc))\n",
    "        \n",
    "        feedback(test_acc)\n",
    "  \n",
    "    # Print best results.\n",
    "    top_results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    print('Top 10 results.')\n",
    "    for i, (trial_id, dna, test_acc) in enumerate(top_results[:10]):\n",
    "        print('#{0:2d} - trial {1:2d} ({2:.3f}): {3}'.format(i + 1, trial_id, test_acc, dna))\n",
    "        \n",
    "        \n",
    "tune(3,2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5704ec-e3ce-46a0-a439-691e9e00f6eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c627f1ee-811b-4a7e-b540-23e0f04ce4a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1: DNA: DNA([0, 0, 0, 0.0005197101697627721, 0])\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_123530/1377846395.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_123530/1377846395.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(max_trials, num_epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "@pg.symbolize\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "    \n",
    "    \n",
    "def train(model, optimizer, input_data, num_epochs):    \n",
    "    tr_x, tr_y, te_x, te_y = input_data\n",
    "\n",
    "    print(f'self.optmizer: {optimizer}')\n",
    "    print(f'self.model: {model}')\n",
    "\n",
    "    model.compile(optimizer=optimizer, \n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "    model.fit(tr_x, tr_y, epochs=num_epochs)\n",
    "\n",
    "    _, test_acc = model.evaluate(te_x, te_y, verbose=2)\n",
    "\n",
    "    return test_acc\n",
    "    \n",
    "    \n",
    "@pg.symbolize\n",
    "class Stacked(object):\n",
    "    def __init__(self, op, repeats):\n",
    "        self.op = op\n",
    "        self.repeats = repeats\n",
    "    \n",
    "    def __call__(self):\n",
    "        print(f'Set op: {self.op} | Set repeats: {self.repeats}')\n",
    "        return Sequential(layers=[tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 28, 28, 1)))]+\\\n",
    "                                 [self.op for _ in range(self.repeats)]+\\\n",
    "                                 [tf.keras.layers.GlobalAveragePooling2D()] +\n",
    "                                 [tf.keras.layers.Dense(10, activation='softmax')])  \n",
    "        \n",
    "\n",
    "\n",
    "def create_trainer():\n",
    "    #st = Stacked()\n",
    "    model = Stacked(op=pg.oneof([Conv2D(pg.oneof([4, 8]), (3, 3)), \n",
    "                                 Conv2D(pg.oneof([4, 8]), (5, 5))]), \n",
    "                    repeats=4)\n",
    "    return Trainer(model=model,\n",
    "                   optimizer=pg.oneof([Adam(2e-2), RMSprop(pg.floatv(1e-6, 1e-3))]))\n",
    "\n",
    "\n",
    "# def nas_model_2(repeats):\n",
    "#     layers = [tf.keras.layers.Flatten()] + \\\n",
    "#              [Dense(pg.oneof([64, 128]), \n",
    "#                     activation=pg.oneof(['relu', 'sigmoid']))] * repeats + \\\n",
    "#              [tf.keras.layers.Dense(10, activation='softmax')]\n",
    "#     return Sequential(layers=layers)\n",
    "\n",
    "\n",
    "def tune(max_trials, num_epochs):\n",
    "    \"\"\"Tune MNIST model via random search.\"\"\"\n",
    "    results = []\n",
    "    input_data = download_and_prep_data()\n",
    "        \n",
    "    for trainer, feedback in pg.sample(pg.hyper.trace(create_trainer), pg.generators.Random(), max_trials):\n",
    "        print('\\n\\n{}: DNA: {}'.format(feedback.id, feedback.dna))\n",
    "        \n",
    "        #print(model)\n",
    "        #test_acc = train_and_eval(model, input_data, num_epochs)\n",
    "        \n",
    "        with trainer() as t:\n",
    "            print(t)\n",
    "            \n",
    "            test_acc = train(trainer.model, trainer.optimizer, input_data, num_epochs)\n",
    "\n",
    "            results.append((feedback.id, feedback.dna, test_acc))\n",
    "\n",
    "            feedback(test_acc)\n",
    "  \n",
    "    # Print best results.\n",
    "    top_results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    print('Top 10 results.')\n",
    "    for i, (trial_id, dna, test_acc) in enumerate(top_results[:10]):\n",
    "        print('#{0:2d} - trial {1:2d} ({2:.3f}): {3}'.format(i + 1, trial_id, test_acc, dna))\n",
    "        \n",
    "        \n",
    "tune(3,2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edd3b5-a0c3-49e0-8164-6573749227a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb57d28-3be1-486b-8bd0-f786dd17d06b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: DNA: DNA(0, [1, 0])\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 4s 1ms/step - loss: 0.2594 - accuracy: 0.9255\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.9664\n",
      "313/313 - 0s - loss: 0.0978 - accuracy: 0.9704\n",
      "Test loss: 0.09777722507715225, accuracy: 0.9703999757766724\n",
      "2: DNA: DNA(0, [0, 1])\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4664 - accuracy: 0.8855\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2226 - accuracy: 0.9363\n",
      "313/313 - 0s - loss: 0.1979 - accuracy: 0.9432\n",
      "Test loss: 0.19792458415031433, accuracy: 0.9431999921798706\n",
      "3: DNA: DNA(1, [1, 0, 0])\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.1430 - accuracy: 0.9579\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0574 - accuracy: 0.9826\n",
      "313/313 - 0s - loss: 0.0625 - accuracy: 0.9800\n",
      "Test loss: 0.06245436519384384, accuracy: 0.9800000190734863\n",
      "Top 10 results.\n",
      "# 1 - trial  3 (0.980): DNA(1, [1, 0, 0])\n",
      "# 2 - trial  1 (0.970): DNA(0, [1, 0])\n",
      "# 3 - trial  2 (0.943): DNA(0, [0, 1])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Add\n",
    "\n",
    "\n",
    "def get_m1():\n",
    "    # NOTE(daiyip): We use a zero-argument `lambda` function to\n",
    "    # wrap each candidate in order to construct a conditional search\n",
    "    # space.\n",
    "    return lambda: [  # pylint: disable=g-long-lambda\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(pg.oneof([64, 128]),\n",
    "                                  pg.oneof(['relu', 'sigmoid']))\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "def get_m2():\n",
    "    return lambda: [tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 28, 28, 1))),\n",
    "                    tf.keras.layers.Conv2D(pg.oneof([64, 128]),\n",
    "                                pg.oneof([(3, 3), (5, 5)]),\n",
    "                                activation=pg.oneof(['relu', 'sigmoid'])),\n",
    "                    tf.keras.layers.Flatten()]\n",
    "\n",
    "\n",
    "\n",
    "# def get_m2(z):\n",
    "#     l = [tf.keras.layers.Conv2D(pg.oneof([64, 128]),\n",
    "#                                 pg.oneof([(3, 3), (5, 5)]),\n",
    "#                                 activation=pg.oneof(['relu', 'sigmoid'])) for l in range(z)]\n",
    "    \n",
    "#     final_list = [tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, 28, 28, 1)))] + \\\n",
    "#                  l + \\\n",
    "#                  [tf.keras.layers.Flatten()]\n",
    "    \n",
    "#     return lambda: final_list\n",
    "\n",
    "\n",
    "def get_final_layer():\n",
    "    return [tf.keras.layers.Dense(10, activation='softmax')]\n",
    "\n",
    "\n",
    "# def create_model() -> tf.keras.Model:\n",
    "#     \"\"\"Create model for training.\n",
    " \n",
    "#     Create a simple tf.keras model for training.\n",
    " \n",
    "#     Returns:\n",
    "#       The model to use for training.\n",
    "#     \"\"\"\n",
    "#     left_branch = tf.keras.Sequential(pg.oneof([get_m1(), get_m2()]))\n",
    "#     right_branch = tf.keras.Sequential(pg.oneof([get_m1(), get_m2()]))\n",
    "#     merged = Concatenate([left_branch, right_branch])\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(merged)\n",
    "#     model.add(get_final_layer())\n",
    "    \n",
    "#     return model\n",
    "                                          \n",
    "\n",
    "def create_model_orig() -> tf.keras.Model:\n",
    "    return Sequential(pg.oneof([get_m1(), get_m2()]) + get_final_layer())\n",
    "\n",
    "\n",
    "def train_and_eval(input_data, num_epochs) -> None:\n",
    "    \"\"\"Run training and evaluation.\"\"\"\n",
    "    tr_x, tr_y, te_x, te_y = input_data\n",
    "    model = create_model_orig()\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(tr_x, tr_y, epochs=num_epochs)\n",
    "    test_loss, test_acc = model.evaluate(te_x, te_y, verbose=2)\n",
    "    print('Test loss: {}, accuracy: {}'.format(test_loss, test_acc))\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def tune(max_trials, num_epochs):\n",
    "    \"\"\"Tune MNIST model via random search.\"\"\"\n",
    "    results = []\n",
    "    input_data = download_and_prep_data()\n",
    "    # NOTE(daiyip): `pg.sample` returns an iterator of (example, feedback_fn)\n",
    "    # from a hyper object (the search space) and a DNAGenerator (the search\n",
    "    # algorithm), with an optional flag to set the max examples to sample.\n",
    "    # Different from defining a search space from symbolized classes or functors,\n",
    "    # `pg.hyper.trace` allows users to define a search space\n",
    "    # without symbolizing user classes, by eagerly executing the user function to\n",
    "    # collect decision points in the search space. Each point in the search space\n",
    "    # is a list of numbers materialized from the decision points in the search\n",
    "    # space. Instead of using these numbers to construct the user program, we can\n",
    "    # directly apply `feedback.dna` to the user function called in the loop via\n",
    "    # `with example():`, which materializes the hyper values used\n",
    "    # within the user function using the decisions made by the search algorithm\n",
    "    # for current trial. As a result, each call to the user function yields\n",
    "    # different hyper-parameters implicitly bound with current trial.\n",
    "    for example, feedback in pg.sample(pg.hyper.trace(create_model_orig), pg.generators.Random(), max_trials):\n",
    "        \n",
    "        print('{}: DNA: {}'.format(feedback.id, feedback.dna))\n",
    "        \n",
    "        with example():\n",
    "            test_acc = train_and_eval(input_data, num_epochs)\n",
    "        \n",
    "        results.append((feedback.id, feedback.dna, test_acc))\n",
    "        \n",
    "        # NOTE: for random generator, following call to `feedback` is a no-op.\n",
    "        # We keep it here in case we want to change algorithm.\n",
    "        feedback(test_acc)\n",
    "\n",
    "    # Print best results.\n",
    "    top_results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    print('Top 10 results.')\n",
    "    for i, (trial_id, dna, test_acc) in enumerate(top_results[:10]):\n",
    "        print('#{0:2d} - trial {1:2d} ({2:.3f}): {3}'.format(i + 1, trial_id, test_acc, dna))\n",
    "\n",
    "\n",
    "tune(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917dd48-8261-4b5f-bfd2-d5cd357a6d73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test 5 - NAS-Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7b65034-0a7c-4663-a885-a50e09260e0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_spec(ops=ManyOf(name=None, hints=0, num_choices=5, candidates=[0: 'conv3x3-bn-relu', 1: 'conv1x1-bn-relu', 2: 'maxpool3x3'], choices_distinct=False, choices_sorted=False, where=None), matrix=[0: [0: 0, 1: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 2: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 3: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 4: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 5: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 6: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None)], 1: [0: 0, 1: 0, 2: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 3: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 4: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 5: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 6: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None)], 2: [0: 0, 1: 0, 2: 0, 3: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 4: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 5: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 6: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None)], 3: [0: 0, 1: 0, 2: 0, 3: 0, 4: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 5: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 6: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None)], 4: [0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None), 6: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None)], 5: [0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: OneOf(name=None, hints=1, num_choices=1, candidates=[0: 0, 1: 1], choices_distinct=True, choices_sorted=False, where=None)], 6: [0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if './nasbench/nasbench' not in sys.path:\n",
    "    sys.path.insert(0, './nasbench/nasbench')\n",
    "\n",
    "\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "#from nasbench.nasbench import api\n",
    "import numpy as np\n",
    "import pyglove as pg\n",
    "\n",
    "# Useful constants\n",
    "INPUT = 'input'\n",
    "OUTPUT = 'output'\n",
    "CONV3X3 = 'conv3x3-bn-relu'\n",
    "CONV1X1 = 'conv1x1-bn-relu'\n",
    "MAXPOOL3X3 = 'maxpool3x3'\n",
    "NUM_VERTICES = 7\n",
    "EDGE_SPOTS = NUM_VERTICES * (NUM_VERTICES - 1) / 2   # Upper triangular matrix\n",
    "OP_SPOTS = NUM_VERTICES - 2   # Input/output vertices are fixed\n",
    "ALLOWED_OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
    "ALLOWED_EDGES = [0, 1]   # Binary adjacency matrix\n",
    "\n",
    "DEFAULT_NAS_BENCH_108_EPOCHS_FILE = '/home/guilherme/data2/doutorado/nasbench-101/data/nasbench_only108.tfrecord'\n",
    "\n",
    "@pg.symbolize([('ops', pg.typing.List(pg.typing.Str())), ('matrix', pg.typing.List(pg.typing.List(pg.typing.Int())))])\n",
    "def model_spec(ops, matrix):\n",
    "    \"\"\"NASBench model spec that is parameterized by ops and their connections.\n",
    "  \n",
    "    Args:\n",
    "      ops: a list of allowed ops except the INPUT and OUTPUT layer.\n",
    "      matrix: the adjacency matrix for the connectivity of each layers, which\n",
    "        should be an upper triangle matrix.\n",
    "  \n",
    "    Returns:\n",
    "      A NASBench spec.\n",
    "    \"\"\"\n",
    "    return api.ModelSpec(matrix=np.array(matrix), ops=[INPUT] + ops + [OUTPUT])\n",
    "\n",
    "\n",
    "# We introduce hints so controller can deal with different knobs differently.\n",
    "OP_HINTS = 0\n",
    "EDGE_HINTS = 1\n",
    "\n",
    "\n",
    "def default_search_space():\n",
    "    \"\"\"The default search space in NAS-Bench.\n",
    "  \n",
    "    This equals to the default search space of NAS-Bench, which mutate candidate\n",
    "    ops and their connections.\n",
    "  \n",
    "    Returns:\n",
    "      A hyper model object that repesents a search space.\n",
    "    \"\"\"\n",
    "    matrix = [\n",
    "        [pg.oneof([0, 1], hints=EDGE_HINTS) if y > x else 0\n",
    "         for y in range(NUM_VERTICES)]\n",
    "        for x in range(NUM_VERTICES)\n",
    "    ]\n",
    "    return model_spec(pg.manyof(NUM_VERTICES - 2, ALLOWED_OPS, choices_distinct=False, hints=OP_HINTS), matrix)\n",
    "\n",
    "\n",
    "default_search_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adee3889-cf09-460b-8f2c-9b90ff08bbc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random(\n",
      "  seed = None\n",
      ")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10003/958146783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m#times, best_valid, best_test = search(nasbench, search_model, algorithm, i, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/pyglove/core/symbolic/functor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mlist_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkeyword_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_type_check_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       return_value = self.signature.return_value.apply(\n",
      "\u001b[0;32m~/data2/anaconda3/envs/icao_nets_training/lib/python3.8/site-packages/pyglove/core/symbolic/functor.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Functor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10003/3858899945.py\u001b[0m in \u001b[0;36mmodel_spec\u001b[0;34m(ops, matrix)\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mNASBench\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mops\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mOUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "# def search(nasbench, search_model, algo, repeat_id, max_train_hours=5e6):\n",
    "#     \"\"\"Define the search procedure.\n",
    "  \n",
    "#     Args:\n",
    "#       nasbench: NASBench object.\n",
    "#       search_model: which is a `model` object annotated with `oneof`.\n",
    "#       algo: algorithm for search.\n",
    "#       repeat_id: identifier of current repeat.\n",
    "#       max_train_hours: max time budget to train the models, which is the sum\n",
    "#         of training time queried from NAS-Bench.\n",
    "  \n",
    "#     Returns:\n",
    "#       A tuple of (total time spent at step i for all steps,\n",
    "#                   best validation accuracy at step i for all steps,\n",
    "#                   best test accuracy at step i for all steps)\n",
    "#     \"\"\"\n",
    "#     #nasbench.reset_budget_counters()\n",
    "#     times, best_valids, best_tests = [0.0], [0.0], [0.0]\n",
    "#     valid_models = 0\n",
    "#     time_spent = 0\n",
    "#     start_time = time.time()\n",
    "#     last_report_time = start_time\n",
    "#     for model, feedback in pg.sample(search_model, algo, name=str(repeat_id)):\n",
    "        #spec = model()\n",
    "        #if nasbench.is_valid(spec):\n",
    "        #    results = nasbench.query(spec)\n",
    "        #    valid_models += 1\n",
    "        #    feedback(results['validation_accuracy'])\n",
    "        #    if results['validation_accuracy'] > best_valids[-1]:\n",
    "        #        best_valids.append(results['validation_accuracy'])\n",
    "        #        best_tests.append(results['test_accuracy'])\n",
    "        #    else:\n",
    "        #        best_valids.append(best_valids[-1])\n",
    "        #        best_tests.append(best_tests[-1])\n",
    "        #    time_spent, _ = nasbench.get_budget_counters()\n",
    "        #    times.append(time_spent)\n",
    "        #    if time_spent > max_train_hours:\n",
    "        #      # Break the first time we exceed the budget.\n",
    "        #        feedback.end_loop()\n",
    "        #        break\n",
    "        #else:\n",
    "        #    feedback.skip()\n",
    "        \n",
    "        #print(model)\n",
    "    \n",
    "        #if feedback.id % 100 == 0:\n",
    "        #    now = time.time()\n",
    "        #    print(f'Tried {feedback.id} models, valid {valid_models}, '\n",
    "        #          f'time_spent {time_spent}, elapse since last report: '\n",
    "        #          f'{now - last_report_time} seconds.')\n",
    "        #    last_report_time = now\n",
    "    #print(f'Total time elapse: {time.time() - start_time} seconds.')\n",
    "    #return times, best_valids, best_tests\n",
    "\n",
    "\n",
    "@pg.symbolize\n",
    "def node_selector(x, hints):\n",
    "    \"\"\"A functor to select node based on hints.\"\"\"\n",
    "    return x.spec.hints == hints\n",
    "\n",
    "\n",
    "def create_search_algorithm(flag_value):\n",
    "    \"\"\"Create search algorithm from flag.\"\"\"\n",
    "    if flag_value == 'random':\n",
    "        return pg.generators.Random()\n",
    "    elif flag_value == 'evolution':\n",
    "        return pg.evolution.regularized_evolution(\n",
    "          mutator=(\n",
    "              pg.evolution.mutators.Uniform(\n",
    "                  where=node_selector(hints=OP_HINTS))         # pylint: disable=no-value-for-parameter\n",
    "              >> pg.evolution.mutators.Uniform(\n",
    "                  where=node_selector(hints=EDGE_HINTS)) ** 3  # pylint: disable=no-value-for-parameter\n",
    "          ),\n",
    "          population_size=50,\n",
    "          tournament_size=10)\n",
    "    else:\n",
    "        return pg.load(flag_value)\n",
    "\n",
    "\n",
    "# Load the dataset.\n",
    "#nasbench = api.NASBench(DEFAULT_NAS_BENCH_108_EPOCHS_FILE)\n",
    "\n",
    "search_model = default_search_space()\n",
    "  \n",
    "#print(search_model)    \n",
    "    \n",
    "# Start search.\n",
    "#for i in range(FLAGS.repeat_start, FLAGS.repeat_end):\n",
    "#    print(f'Repeat #{i}')\n",
    "    # Create algorithm.\n",
    "algorithm = create_search_algorithm('random')\n",
    "\n",
    "print(algorithm)\n",
    "\n",
    "for model, feedback in pg.sample(search_model, algorithm, name='1'):\n",
    "    spec = model()\n",
    "    \n",
    "    #times, best_valid, best_test = search(nasbench, search_model, algorithm, i, 1)\n",
    "  \n",
    "    #print('%15s %15s %15s %15s' % ('# trials','best valid','best test','simulated train hours'))\n",
    "    #print('%15d %15.4f %15.4f %15d' % (len(times), best_valid[-1], best_test[-1], times[-1]))\n",
    "  \n",
    "    # if FLAGS.output_dir:\n",
    "    #     pg.Dict(times=times, best_valid=best_valid, best_test=best_test).save(os.path.join(FLAGS.output_dir, f'repeat_{i}.json'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5fcd7-e048-4a79-a0b3-9225a87d883e",
   "metadata": {},
   "source": [
    "# Test 6 - NATS-Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bf04fdb-0f7c-433c-b2d7-d9dac92437e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-13 01:49:25] Try to use the default NATS-Bench (size) path from fast_mode=True and path=None.\n",
      "{'candidates': [8, 16, 24, 32, 40, 48, 56, 64], 'num_layers': 5}\n",
      "56:16:16:64:16\n",
      "48:64:32:40:16\n",
      "64:8:24:48:32\n",
      "40:16:40:8:40\n",
      "32:24:48:56:16\n",
      "40:64:16:40:56\n",
      "32:8:64:32:8\n",
      "16:32:40:48:8\n",
      "32:56:64:40:40\n",
      "56:16:24:32:48\n",
      "56:40:40:56:24\n",
      "32:40:24:32:56\n",
      "40:64:48:56:8\n",
      "56:24:56:32:48\n",
      "24:16:56:40:24\n",
      "16:16:16:16:8\n",
      "56:24:64:32:40\n",
      "24:40:24:56:24\n",
      "32:16:64:48:16\n",
      "8:8:40:32:32\n",
      "56:40:48:48:48\n",
      "16:48:16:40:64\n",
      "24:56:56:24:8\n",
      "56:48:40:48:40\n",
      "40:32:56:16:16\n",
      "16:48:16:48:8\n",
      "8:40:56:48:48\n",
      "24:24:8:64:8\n",
      "32:40:48:48:8\n",
      "16:8:8:32:48\n",
      "56:32:32:8:32\n",
      "8:64:32:56:32\n",
      "56:32:16:40:40\n",
      "24:24:24:40:8\n",
      "8:24:16:8:32\n",
      "16:8:64:40:40\n",
      "64:64:48:40:32\n",
      "8:16:8:40:56\n",
      "24:32:56:48:56\n",
      "48:48:32:48:56\n",
      "40:24:40:16:32\n",
      "48:8:56:32:32\n",
      "8:8:24:32:16\n",
      "16:8:16:40:48\n",
      "56:32:16:40:64\n",
      "40:8:8:24:24\n",
      "48:56:8:56:56\n",
      "48:8:24:40:56\n",
      "64:64:8:8:8\n",
      "24:64:56:24:16\n",
      "56:48:16:40:64\n",
      "24:32:56:40:56\n",
      "32:56:32:40:40\n",
      "64:64:48:40:40\n",
      "48:16:56:40:48\n",
      "64:32:48:16:32\n",
      "56:32:40:48:40\n",
      "24:16:56:48:56\n",
      "64:64:48:24:40\n",
      "32:32:8:8:56\n",
      "24:32:40:40:56\n",
      "64:64:48:24:32\n",
      "64:64:24:40:32\n",
      "24:32:56:40:56\n",
      "24:64:48:40:32\n",
      "64:16:48:40:32\n",
      "48:16:56:48:48\n",
      "56:24:40:48:40\n",
      "Total time elapse: 0.8967981338500977 seconds.\n",
      "       # trials  best valid (%)   best test (%) simulated train hours\n",
      "             68         83.8840         92.4100                  3644\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import nats_bench\n",
    "import pyglove as pg\n",
    "\n",
    "\n",
    "DEFAULT_NATS_FILEs = dict(tss=None, sss=None)\n",
    "\n",
    "# Results in the paper use reporting epochs $H^1$ and $H^2$ for the topology\n",
    "# and size search spaces respectively. See section 3.3 of the paper.\n",
    "DEFAULT_REPORTING_EPOCH = dict(tss=200, sss=90)\n",
    "VALIDATION_SET_REPORTING_EPOCH = 12\n",
    "\n",
    "\n",
    "@pg.functor([('ops', pg.typing.List(pg.typing.Str())),('num_nodes', pg.typing.Int())])\n",
    "def model_tss_spc(ops, num_nodes):\n",
    "    \"\"\"The architecture in the topology search space of NATS-Bench.\"\"\"\n",
    "    nodes, k = [], 0\n",
    "    for i in range(1, num_nodes):\n",
    "        xstrs = []\n",
    "        for j in range(i):\n",
    "            xstrs.append('{:}~{:}'.format(ops[k], j))\n",
    "            k += 1\n",
    "        nodes.append('|' + '|'.join(xstrs) + '|')\n",
    "    return '+'.join(nodes)\n",
    "\n",
    "\n",
    "@pg.functor([('channels', pg.typing.List(pg.typing.Int()))])\n",
    "def model_sss_spc(channels):\n",
    "    \"\"\"The architecture in the size search space of NATS-Bench.\"\"\"\n",
    "    return ':'.join(str(x) for x in channels)\n",
    "\n",
    "\n",
    "def get_search_space(ss_indicator):\n",
    "    \"\"\"The default search space in NATS-Bench.\n",
    "  \n",
    "    Args:\n",
    "      ss_indicator: tss or sss, indicating the topology or size search space.\n",
    "  \n",
    "    Returns:\n",
    "      A hyper model object that repesents a search space.\n",
    "    \"\"\"\n",
    "    info = nats_bench.search_space_info('nats-bench', ss_indicator)\n",
    "    print(info)\n",
    "    if ss_indicator == 'tss':\n",
    "        total = info['num_nodes'] * (info['num_nodes'] - 1) // 2\n",
    "        return model_tss_spc(pg.sublist_of(total, info['op_names'], choices_distinct=False), info['num_nodes'])\n",
    "    elif ss_indicator == 'sss':\n",
    "        return model_sss_spc(pg.sublist_of(info['num_layers'], info['candidates'], choices_distinct=False))\n",
    "\n",
    "\n",
    "def search(nats_api,\n",
    "           search_model,\n",
    "           algo,\n",
    "           dataset='cifar10',\n",
    "           reporting_epoch=12,\n",
    "           max_train_hours=2e4):\n",
    "    \"\"\"Define the search procedure.\n",
    "  \n",
    "    Args:\n",
    "      nats_api: the NATS-Bench object.\n",
    "      search_model: which is a `model` object annotated with `one_of`.\n",
    "      algo: algorithm for search.\n",
    "      dataset: the target dataset\n",
    "      reporting_epoch: Use test set results for models trained for this many epochs.\n",
    "      max_train_hours: max time budget to train the models, which is the sum of training time queried from NAS-Bench.\n",
    "  \n",
    "    Returns:\n",
    "      A tuple of (total time spent at step i for all steps,\n",
    "                  best validation accuracy at step i for all steps,\n",
    "                  best test accuracy at step i for all steps)\n",
    "    \"\"\"\n",
    "    nats_api.reset_time()\n",
    "    times, best_valids, best_tests = [0.0], [0.0], [0.0]\n",
    "    valid_models = 0\n",
    "    time_spent = 0\n",
    "    start_time = time.time()\n",
    "    last_report_time = start_time\n",
    "    for model, feedback in pg.sample(search_model, algo):\n",
    "        spec = model()\n",
    "        \n",
    "        print(spec)\n",
    "        \n",
    "        (validation_accuracy, _, _, _) = nats_api.simulate_train_eval(spec, dataset=dataset, hp=VALIDATION_SET_REPORTING_EPOCH)\n",
    "        \n",
    "        time_spent = nats_api.used_time\n",
    "        \n",
    "        more_info = nats_api.get_more_info(spec, dataset, hp=reporting_epoch)  # pytype: disable=wrong-arg-types  # dict-kwargs\n",
    "        \n",
    "        valid_models += 1\n",
    "        \n",
    "        feedback(validation_accuracy)\n",
    "        \n",
    "        if validation_accuracy > best_valids[-1]:\n",
    "            best_valids.append(validation_accuracy)\n",
    "            best_tests.append(more_info['test-accuracy'])\n",
    "        else:\n",
    "            best_valids.append(best_valids[-1])\n",
    "            best_tests.append(best_tests[-1])\n",
    "\n",
    "        times.append(time_spent)\n",
    "        time_spent_in_hours = time_spent / (60 * 60)\n",
    "        \n",
    "        if time_spent_in_hours > max_train_hours:\n",
    "            break # Break the first time we exceed the budget.\n",
    "        \n",
    "        if feedback.id % 100 == 0:\n",
    "            now = time.time()\n",
    "            print(f'Tried {feedback.id} models, valid {valid_models}, '\n",
    "                  f'time_spent_in_hours: {int(time_spent_in_hours)}h, '\n",
    "                  f'time_spent: {round(time_spent,3)}s, '\n",
    "                  f'elapse since last report: {round(now - last_report_time,3)}s.')\n",
    "            last_report_time = now\n",
    "            \n",
    "    print(f'Total time elapse: {time.time() - start_time} seconds.')\n",
    "    # Remove the first element of each list because these are placeholders\n",
    "    # used for computing the current max. They don't correspond to\n",
    "    # actual results from nats_api.\n",
    "    return times[1:], best_valids[1:], best_tests[1:]\n",
    "\n",
    "\n",
    "def get_algorithm(algorithm_str):\n",
    "    \"\"\"Creates algorithm.\"\"\"\n",
    "    if algorithm_str == 'random':\n",
    "        return pg.generators.Random()\n",
    "    elif algorithm_str == 'evolution':\n",
    "        return pg.evolution.regularized_evolution(mutator=pg.evolution.mutators.Uniform(), population_size=50, tournament_size=10)\n",
    "    else:\n",
    "        return pg.load(algorithm_str)\n",
    "\n",
    "\n",
    "SEARCH_SPACE = 'sss'    \n",
    "    \n",
    "# Load the dataset.\n",
    "nats_bench.api_utils.reset_file_system('default')\n",
    "nats_api = nats_bench.create(DEFAULT_NATS_FILEs[SEARCH_SPACE], SEARCH_SPACE, fast_mode=True, verbose=False)\n",
    "\n",
    "# Create search space.\n",
    "search_model = get_search_space(SEARCH_SPACE)\n",
    "reporting_epoch = DEFAULT_REPORTING_EPOCH[SEARCH_SPACE]\n",
    "\n",
    "algorithm = get_algorithm('evolution')\n",
    "\n",
    "times, best_valid, best_test = search(nats_api, search_model, algorithm, 'cifar10', reporting_epoch, max_train_hours=1)\n",
    "\n",
    "print('%15s %15s %15s %15s' % ('# trials', 'best valid (%)', 'best test (%)', 'simulated train hours'))\n",
    "print('%15d %15.4f %15.4f %21d' % (len(times), best_valid[-1], best_test[-1], times[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1a9fd-18b7-4bb3-9c9d-ff3cdaec64e6",
   "metadata": {},
   "source": [
    "# Test 7 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96ccd437-fc39-4432-b190-34686a1a04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  0 : 2,\n",
      "  1 : 5,\n",
      "  2 : 1,\n",
      "  3 : 4\n",
      "]\n",
      "Architecture 0: {'n_denses_0': 2, 'n_denses_1': 5, 'n_denses_2': 1, 'n_denses_3': 4}\n",
      "[\n",
      "  0 : 2,\n",
      "  1 : 5,\n",
      "  2 : 4,\n",
      "  3 : 5\n",
      "]\n",
      "Architecture 1: {'n_denses_0': 2, 'n_denses_1': 5, 'n_denses_2': 4, 'n_denses_3': 5}\n",
      "[\n",
      "  0 : 2,\n",
      "  1 : 2,\n",
      "  2 : 5,\n",
      "  3 : 3\n",
      "]\n",
      "Architecture 2: {'n_denses_0': 2, 'n_denses_1': 2, 'n_denses_2': 5, 'n_denses_3': 3}\n",
      "[\n",
      "  0 : 4,\n",
      "  1 : 2,\n",
      "  2 : 4,\n",
      "  3 : 3\n",
      "]\n",
      "Architecture 3: {'n_denses_0': 4, 'n_denses_1': 2, 'n_denses_2': 4, 'n_denses_3': 3}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import nats_bench\n",
    "import pyglove as pg\n",
    "\n",
    "\n",
    "DEFAULT_NATS_FILEs = dict(tss=None, sss=None)\n",
    "\n",
    "# Results in the paper use reporting epochs $H^1$ and $H^2$ for the topology\n",
    "# and size search spaces respectively. See section 3.3 of the paper.\n",
    "DEFAULT_REPORTING_EPOCH = dict(tss=200, sss=90)\n",
    "VALIDATION_SET_REPORTING_EPOCH = 12\n",
    "\n",
    "\n",
    "@pg.functor([('channels', pg.typing.List(pg.typing.Int()))])\n",
    "def model_sss_spc(channels):\n",
    "    print(channels)\n",
    "    \"\"\"The architecture in the size search space of NATS-Bench.\"\"\"\n",
    "    return ':'.join(str(x) for x in channels)\n",
    "\n",
    "\n",
    "@pg.functor([('n_denses', pg.typing.List(pg.typing.Int()))])\n",
    "def model_1_spc(n_denses):\n",
    "    print(n_denses)\n",
    "    return {f'n_denses_{idx}':x for idx,x in enumerate(n_denses)}\n",
    "\n",
    "\n",
    "def get_search_space(ss_indicator):\n",
    "    \"\"\"The default search space in NATS-Bench.\n",
    "  \n",
    "    Args:\n",
    "      ss_indicator: tss or sss, indicating the topology or size search space.\n",
    "  \n",
    "    Returns:\n",
    "      A hyper model object that repesents a search space.\n",
    "    \"\"\"\n",
    "    #info = nats_bench.search_space_info('nats-bench', ss_indicator)\n",
    "    #print(info)  # 'candidates': [8, 16, 24, 32, 40, 48, 56, 64], 'num_layers': 5\n",
    "    if ss_indicator == 'tss':\n",
    "        total = info['num_nodes'] * (info['num_nodes'] - 1) // 2\n",
    "        return model_tss_spc(pg.sublist_of(total, info['op_names'], choices_distinct=False), info['num_nodes'])\n",
    "    elif ss_indicator == 'sss':\n",
    "        #return model_sss_spc(pg.sublist_of(info['num_layers'], info['candidates'], choices_distinct=False))\n",
    "        return model_sss_spc(pg.sublist_of(5, [8, 16, 24, 32, 40, 48, 56, 64], choices_distinct=False))\n",
    "    elif ss_indicator == 'ss_1':\n",
    "        return model_1_spc(pg.sublist_of(4, [1, 2, 3, 4, 5], choices_distinct=False))\n",
    "    \n",
    "\n",
    "\n",
    "def sim_train_eval(model):\n",
    "    return random.random()\n",
    "    \n",
    "    \n",
    "    \n",
    "def search(nats_api,\n",
    "           search_model,\n",
    "           algo,\n",
    "           dataset='cifar10',\n",
    "           reporting_epoch=12,\n",
    "           max_train_hours=2e4):\n",
    "    \"\"\"Define the search procedure.\n",
    "  \n",
    "    Args:\n",
    "      nats_api: the NATS-Bench object.\n",
    "      search_model: which is a `model` object annotated with `one_of`.\n",
    "      algo: algorithm for search.\n",
    "      dataset: the target dataset\n",
    "      reporting_epoch: Use test set results for models trained for this many epochs.\n",
    "      max_train_hours: max time budget to train the models, which is the sum of training time queried from NAS-Bench.\n",
    "  \n",
    "    Returns:\n",
    "      A tuple of (total time spent at step i for all steps,\n",
    "                  best validation accuracy at step i for all steps,\n",
    "                  best test accuracy at step i for all steps)\n",
    "    \"\"\"\n",
    "    #nats_api.reset_time()\n",
    "    times, best_valids, best_tests = [0.0], [0.0], [0.0]\n",
    "    valid_models = 0\n",
    "    time_spent = 0\n",
    "    start_time = time.time()\n",
    "    last_report_time = start_time\n",
    "    cnt = 0\n",
    "    for model, feedback in pg.sample(search_model, algo):\n",
    "        spec = model()\n",
    "        \n",
    "        print(f'Architecture {cnt}: {spec}')\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "        if cnt == 4:\n",
    "            break\n",
    "        \n",
    "#         (validation_accuracy, _, _, _) = nats_api.simulate_train_eval(spec, dataset=dataset, hp=VALIDATION_SET_REPORTING_EPOCH)\n",
    "        validation_accuracy = sim_train_eval(spec)\n",
    "    \n",
    "#         time_spent = nats_api.used_time\n",
    "        \n",
    "#         more_info = nats_api.get_more_info(spec, dataset, hp=reporting_epoch)  # pytype: disable=wrong-arg-types  # dict-kwargs\n",
    "        \n",
    "#         valid_models += 1\n",
    "        \n",
    "        feedback(validation_accuracy)\n",
    "        \n",
    "#         if validation_accuracy > best_valids[-1]:\n",
    "#             best_valids.append(validation_accuracy)\n",
    "#             best_tests.append(more_info['test-accuracy'])\n",
    "#         else:\n",
    "#             best_valids.append(best_valids[-1])\n",
    "#             best_tests.append(best_tests[-1])\n",
    "\n",
    "#         times.append(time_spent)\n",
    "#         time_spent_in_hours = time_spent / (60 * 60)\n",
    "        \n",
    "#         if time_spent_in_hours > max_train_hours:\n",
    "#             break # Break the first time we exceed the budget.\n",
    "        \n",
    "#         if feedback.id % 100 == 0:\n",
    "#             now = time.time()\n",
    "#             print(f'Tried {feedback.id} models, valid {valid_models}, '\n",
    "#                   f'time_spent_in_hours: {int(time_spent_in_hours)}h, '\n",
    "#                   f'time_spent: {round(time_spent,3)}s, '\n",
    "#                   f'elapse since last report: {round(now - last_report_time,3)}s.')\n",
    "#             last_report_time = now\n",
    "            \n",
    "    # print(f'Total time elapse: {time.time() - start_time} seconds.')\n",
    "    # Remove the first element of each list because these are placeholders\n",
    "    # used for computing the current max. They don't correspond to\n",
    "    # actual results from nats_api.\n",
    "    # return times[1:], best_valids[1:], best_tests[1:]\n",
    "\n",
    "\n",
    "def get_algorithm(algorithm_str):\n",
    "    \"\"\"Creates algorithm.\"\"\"\n",
    "    if algorithm_str == 'random':\n",
    "        return pg.generators.Random()\n",
    "    elif algorithm_str == 'evolution':\n",
    "        return pg.evolution.regularized_evolution(mutator=pg.evolution.mutators.Uniform(), population_size=50, tournament_size=10)\n",
    "    else:\n",
    "        return pg.load(algorithm_str)\n",
    "\n",
    "\n",
    "ss_indicator = 'ss_1'    \n",
    "    \n",
    "# Load the dataset.\n",
    "#nats_bench.api_utils.reset_file_system('default')\n",
    "#nats_api = nats_bench.create(DEFAULT_NATS_FILEs[ss_indicator], ss_indicator, fast_mode=True, verbose=False)\n",
    "nats_api = None\n",
    "                           \n",
    "# Create search space.\n",
    "search_model = get_search_space(ss_indicator)\n",
    "\n",
    "algorithm = get_algorithm('evolution')\n",
    "\n",
    "#times, best_valid, best_test = search(nats_api, search_model, algorithm, dataset='cifar10', max_train_hours=0.05)\n",
    "search(nats_api, search_model, algorithm, dataset='cifar10', max_train_hours=0.05)\n",
    "\n",
    "#print('%15s %15s %15s %15s' % ('# trials', 'best valid (%)', 'best test (%)', 'simulated train hours'))\n",
    "#print('%15d %15.4f %15.4f %21d' % (len(times), best_valid[-1], best_test[-1], times[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
