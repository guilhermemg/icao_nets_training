{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import neptune\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as prep_input_mobilenetv2\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "if '../../../notebooks/' not in sys.path:\n",
    "    sys.path.append('../../../notebooks/')\n",
    "\n",
    "import utils.constants as cts\n",
    "import utils.draw_utils as dr\n",
    "\n",
    "from models.oface_mouth_model import OpenfaceMouth\n",
    "\n",
    "from data_loaders.data_loader import DLName\n",
    "\n",
    "from net_data_loaders.net_data_loader import NetDataLoader\n",
    "\n",
    "# from gt_loaders.gen_gt import Eval\n",
    "# from gt_loaders.fvc_gt import FVC_GTLoader\n",
    "# from gt_loaders.pybossa_gt import PybossaGTLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict GPU Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict memory growth -------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try: \n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "except: \n",
    "    raise Exception(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
    "\n",
    "## restrict memory growth -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Experiment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Neptune\n"
     ]
    }
   ],
   "source": [
    "print('Starting Neptune')\n",
    "neptune.init('guilhermemg/icao-nets-training')    \n",
    "    \n",
    "def log_data(logs):\n",
    "    neptune.log_metric('epoch_accuracy', logs['accuracy'])\n",
    "    neptune.log_metric('epoch_val_accuracy', logs['val_accuracy'])\n",
    "    neptune.log_metric('epoch_loss', logs['loss'])    \n",
    "    neptune.log_metric('epoch_val_loss', logs['val_loss'])    \n",
    "\n",
    "    \n",
    "def lr_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        new_lr = PARAMS['learning_rate']\n",
    "    else:\n",
    "        new_lr = PARAMS['learning_rate'] * np.exp(0.1 * ((epoch//50)*50 - epoch))\n",
    "\n",
    "    neptune.log_metric('learning_rate', new_lr)\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "m = OpenfaceMouth()\n",
    "req = cts.ICAO_REQ.MOUTH\n",
    "dl_names = [DLName.FVC_PYBOSSA]\n",
    "print(f'DL names: {dl_names}')\n",
    "\n",
    "print('Loading data')\n",
    "netDataLoader = NetDataLoader(m, req, dl_names, True)\n",
    "in_data = netDataLoader.load_data()\n",
    "# in_data = pd.read_csv(cts.LABELS_FQA_SCORES_DATA_PATH)\n",
    "# in_data = in_data[in_data.fqa_score >= 0.5]\n",
    "# in_data = in_data.astype({'comp':'str'})\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 9639\n",
      "Starting data generators\n",
      "Starting data generators\n",
      "Found 6940 validated image filenames belonging to 2 classes.\n",
      "Found 1735 validated image filenames belonging to 2 classes.\n",
      "Found 964 validated image filenames belonging to 2 classes.\n",
      "TOTAL: 9639\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PROP = 0.8\n",
    "VALID_PROP = 0.1\n",
    "TEST_PROP = 0.1\n",
    "SEED = 42\n",
    "\n",
    "print(f'N: {len(in_data)}')\n",
    "\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 30\n",
    "BS = 32\n",
    "SHUFFLE = True\n",
    "DROPOUT = 0.5\n",
    "# EARLY_STOPPING = 10\n",
    "OPTIMIZER = 'Adam'\n",
    "DENSE_UNITS = 128\n",
    "\n",
    "print('Starting data generators')\n",
    "train_valid_df = in_data.sample(frac=TRAIN_PROP+VALID_PROP, random_state=SEED)\n",
    "test_df = in_data[~in_data.img_name.isin(train_valid_df.img_name)]\n",
    "\n",
    "print('Starting data generators')\n",
    "datagen = ImageDataGenerator(preprocessing_function=prep_input_mobilenetv2, \n",
    "                             validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(train_valid_df, \n",
    "                                        x_col=\"img_name\", \n",
    "                                        y_col=\"comp\",\n",
    "                                        target_size=(224, 224),\n",
    "                                        class_mode=\"binary\",\n",
    "                                        batch_size=BS, \n",
    "                                        subset='training')\n",
    "\n",
    "validation_gen = datagen.flow_from_dataframe(train_valid_df,\n",
    "                                            x_col=\"img_name\", \n",
    "                                            y_col=\"comp\",\n",
    "                                            target_size=(224, 224),\n",
    "                                            class_mode=\"binary\",\n",
    "                                            batch_size=BS, \n",
    "                                            subset='validation')\n",
    "\n",
    "test_gen = datagen.flow_from_dataframe(test_df,\n",
    "                                       x_col=\"img_name\", \n",
    "                                       y_col=\"comp\",\n",
    "                                       target_size=(224, 224),\n",
    "                                       class_mode=\"binary\",\n",
    "                                       batch_size=BS)\n",
    "\n",
    "print(f'TOTAL: {train_gen.n + validation_gen.n + test_gen.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment\n",
      "https://ui.neptune.ai/guilhermemg/icao-nets-training/e/IC-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(IC-20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters\n",
    "PARAMS = {'batch_size': BS,\n",
    "          'n_epochs': EPOCHS,\n",
    "          'shuffle': SHUFFLE,\n",
    "          'dense_units': DENSE_UNITS,\n",
    "          'learning_rate': INIT_LR,\n",
    "          'optimizer': OPTIMIZER,\n",
    "          'dropout': DROPOUT,\n",
    "#           'early_stopping': EARLY_STOPPING,\n",
    "          'train_prop': TRAIN_PROP,\n",
    "          'validation_prop': VALID_PROP,\n",
    "          'test_prop': TEST_PROP,\n",
    "          'n_train': train_gen.n,\n",
    "          'n_validation': validation_gen.n,\n",
    "          'n_test': test_gen.n,\n",
    "          'seed': SEED}\n",
    "\n",
    "\n",
    "print('Creating experiment')\n",
    "neptune.create_experiment(name='train_mobilenetv2',\n",
    "                          params=PARAMS,\n",
    "                          properties={\n",
    "                                      'dl_names': str([dl_name.value for dl_name in dl_names]),\n",
    "                                      'dl_aligned': True,\n",
    "                                      'icao_req': req.value,\n",
    "                                      'tagger_model': m.get_model_name().value},\n",
    "                          description='Changing learning rate scheduler function to improve results from experiment IC-19',\n",
    "                          tags=['mobilenetv2'],\n",
    "                          upload_source_files=['train_mobilenetv2.py']\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 184s 853ms/step - loss: 0.7590 - accuracy: 0.4912 - val_loss: 0.6965 - val_accuracy: 0.5122\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6974 - accuracy: 0.5083 - val_loss: 0.6941 - val_accuracy: 0.5104\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6945 - accuracy: 0.5151 - val_loss: 0.6936 - val_accuracy: 0.5174\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 15s 70ms/step - loss: 0.6938 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.5064\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6935 - accuracy: 0.4965 - val_loss: 0.6933 - val_accuracy: 0.4971\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6934 - accuracy: 0.4938 - val_loss: 0.6933 - val_accuracy: 0.4948\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6933 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.4873\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 15s 67ms/step - loss: 0.6933 - accuracy: 0.4933 - val_loss: 0.6932 - val_accuracy: 0.4815\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4828 - val_loss: 0.6932 - val_accuracy: 0.4774\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4754 - val_loss: 0.6932 - val_accuracy: 0.4734\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6932 - accuracy: 0.4670 - val_loss: 0.6932 - val_accuracy: 0.4711\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4747 - val_loss: 0.6932 - val_accuracy: 0.4682\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 15s 70ms/step - loss: 0.6932 - accuracy: 0.4761 - val_loss: 0.6932 - val_accuracy: 0.4647\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 15s 70ms/step - loss: 0.6932 - accuracy: 0.4693 - val_loss: 0.6932 - val_accuracy: 0.4641\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4631 - val_loss: 0.6932 - val_accuracy: 0.4635\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4608 - val_loss: 0.6932 - val_accuracy: 0.4659\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4634 - val_loss: 0.6932 - val_accuracy: 0.4618\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 15s 70ms/step - loss: 0.6932 - accuracy: 0.4573 - val_loss: 0.6932 - val_accuracy: 0.4635\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 15s 70ms/step - loss: 0.6932 - accuracy: 0.4640 - val_loss: 0.6932 - val_accuracy: 0.4635\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6932 - accuracy: 0.4612 - val_loss: 0.6932 - val_accuracy: 0.4630\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4599 - val_loss: 0.6932 - val_accuracy: 0.4566\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4587 - val_loss: 0.6932 - val_accuracy: 0.4618\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4511 - val_loss: 0.6932 - val_accuracy: 0.4572\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4622 - val_loss: 0.6932 - val_accuracy: 0.4606\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4558 - val_loss: 0.6932 - val_accuracy: 0.4572\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6932 - accuracy: 0.4556 - val_loss: 0.6932 - val_accuracy: 0.4583\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 15s 67ms/step - loss: 0.6932 - accuracy: 0.4519 - val_loss: 0.6932 - val_accuracy: 0.4566\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6932 - accuracy: 0.4509 - val_loss: 0.6932 - val_accuracy: 0.4549\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 15s 68ms/step - loss: 0.6932 - accuracy: 0.4518 - val_loss: 0.6932 - val_accuracy: 0.4641\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 15s 69ms/step - loss: 0.6932 - accuracy: 0.4632 - val_loss: 0.6932 - val_accuracy: 0.4583\n"
     ]
    }
   ],
   "source": [
    "print('Training network')\n",
    "\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224,224,3),\n",
    "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(PARAMS['dense_units'], activation=\"relu\")(headModel)\n",
    "headModel = Dropout(PARAMS['dropout'])(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False\n",
    "\n",
    "# compile our model\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Log model summary\n",
    "model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "\n",
    "# train the head of the network\n",
    "H = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_gen.n // BS,\n",
    "        validation_data=validation_gen,\n",
    "        validation_steps=validation_gen.n // BS,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[LambdaCallback(on_epoch_end = lambda epoch, logs: log_data(logs)),\n",
    "#                    EarlyStopping(patience=PARAMS['early_stopping'], monitor='accuracy', restore_best_weights=True),\n",
    "                   LearningRateScheduler(lr_scheduler)\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Trained Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Saving model')\n",
    "# Log model weights\n",
    "with tempfile.TemporaryDirectory(dir='.') as d:\n",
    "    prefix = os.path.join(d, 'model_weights')\n",
    "    model.save_weights(os.path.join(prefix, 'model'))\n",
    "    for item in os.listdir(prefix):\n",
    "        neptune.log_artifact(os.path.join(prefix, item), os.path.join('model_weights', item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Trained Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NON_COMP       0.57      0.27      0.36       562\n",
      "        COMP       0.41      0.71      0.52       402\n",
      "\n",
      "    accuracy                           0.45       964\n",
      "   macro avg       0.49      0.49      0.44       964\n",
      "weighted avg       0.50      0.45      0.43       964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Trained Model\")\n",
    "\n",
    "# make predictions on the testing set\n",
    "predIdxs = model.predict(test_gen, batch_size=BS)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(test_gen.labels, predIdxs, target_names=['NON_COMP','COMP']))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating model')\n",
    "eval_metrics = model.evaluate(test_gen, verbose=0)\n",
    "for j, metric in enumerate(eval_metrics):\n",
    "    neptune.log_metric('eval_' + model.metrics_names[j], metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing Experiment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing Neptune\n"
     ]
    }
   ],
   "source": [
    "print('Finishing Neptune')\n",
    "neptune.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "neptune": {
   "notebookId": "d6d95e8a-b251-40a1-bf9d-610ebc484f63"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
