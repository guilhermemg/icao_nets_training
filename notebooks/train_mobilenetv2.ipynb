{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import neptune\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as prep_input_mobilenetv2\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "if '../../../notebooks/' not in sys.path:\n",
    "    sys.path.append('../../../notebooks/')\n",
    "\n",
    "import utils.constants as cts\n",
    "import utils.draw_utils as dr\n",
    "\n",
    "from models.oface_mouth_model import OpenfaceMouth\n",
    "\n",
    "from data_loaders.data_loader import DLName\n",
    "\n",
    "from net_data_loaders.net_data_loader import NetDataLoader\n",
    "\n",
    "# from gt_loaders.gen_gt import Eval\n",
    "# from gt_loaders.fvc_gt import FVC_GTLoader\n",
    "# from gt_loaders.pybossa_gt import PybossaGTLoader\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict GPU Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict memory growth -------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try: \n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "except: \n",
    "    raise Exception(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
    "\n",
    "## restrict memory growth -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Experiment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Neptune')\n",
    "neptune.init('guilhermemg/icao-nets-training')    \n",
    "    \n",
    "def log_data(logs):\n",
    "    neptune.log_metric('epoch_accuracy', logs['accuracy'])\n",
    "    neptune.log_metric('epoch_val_accuracy', logs['val_accuracy'])\n",
    "    neptune.log_metric('epoch_loss', logs['loss'])    \n",
    "    neptune.log_metric('epoch_val_loss', logs['val_loss']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        new_lr = PARAMS['learning_rate']\n",
    "    else:\n",
    "        new_lr = PARAMS['learning_rate'] * np.exp(0.1 * ((epoch//50)*50 - epoch))\n",
    "\n",
    "    #neptune.log_metric('learning_rate', new_lr)\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL names: [<DLName.FVC_PYBOSSA: 'fvc_pybossa'>]\n",
      "Loading data\n",
      "Input data.shape: (5780, 4)\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "m = OpenfaceMouth()\n",
    "req = cts.ICAO_REQ.MOUTH\n",
    "dl_names = [DLName.FVC_PYBOSSA]\n",
    "print(f'DL names: {dl_names}')\n",
    "\n",
    "print('Loading data')\n",
    "netDataLoader = NetDataLoader(m, req, dl_names, True)\n",
    "in_data = netDataLoader.load_data()\n",
    "# in_data = pd.read_csv(cts.LABELS_FQA_SCORES_DATA_PATH)\n",
    "# in_data = in_data[in_data.fqa_score >= 0.5]\n",
    "# in_data = in_data.astype({'comp':'str'})\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>img_name</th>\n",
       "      <th>comp</th>\n",
       "      <th>aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fvc_pybossa</td>\n",
       "      <td>/home/guilherme/data/Dropbox/Link to Desktop/D...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fvc_pybossa</td>\n",
       "      <td>/home/guilherme/data/Dropbox/Link to Desktop/D...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fvc_pybossa</td>\n",
       "      <td>/home/guilherme/data/Dropbox/Link to Desktop/D...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fvc_pybossa</td>\n",
       "      <td>/home/guilherme/data/Dropbox/Link to Desktop/D...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fvc_pybossa</td>\n",
       "      <td>/home/guilherme/data/Dropbox/Link to Desktop/D...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        origin                                           img_name comp aligned\n",
       "0  fvc_pybossa  /home/guilherme/data/Dropbox/Link to Desktop/D...  1.0    True\n",
       "1  fvc_pybossa  /home/guilherme/data/Dropbox/Link to Desktop/D...  0.0    True\n",
       "2  fvc_pybossa  /home/guilherme/data/Dropbox/Link to Desktop/D...  0.0    True\n",
       "3  fvc_pybossa  /home/guilherme/data/Dropbox/Link to Desktop/D...  0.0    True\n",
       "4  fvc_pybossa  /home/guilherme/data/Dropbox/Link to Desktop/D...  1.0    True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4568 1212\n"
     ]
    }
   ],
   "source": [
    "print(in_data[in_data.comp == '1.0'].shape[0], in_data[in_data.comp == '0.0'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4568, 4) (1212, 4)\n",
      "1212\n",
      "(2424, 4)\n",
      "1212\n",
      "1212\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame()\n",
    "\n",
    "df_comp = in_data[in_data.comp == '1.0']\n",
    "df_non_comp = in_data[in_data.comp == '0.0']\n",
    "\n",
    "print(df_comp.shape, df_non_comp.shape)\n",
    "\n",
    "n_imgs = df_non_comp.shape[0]\n",
    "print(n_imgs)\n",
    "\n",
    "df_comp = df_comp[:n_imgs].copy()\n",
    "\n",
    "final_df = final_df.append(df_comp)\n",
    "final_df = final_df.append(df_non_comp)\n",
    "\n",
    "print(final_df.shape)\n",
    "print(final_df[final_df.comp == '1.0'].shape[0])\n",
    "print(final_df[final_df.comp == '0.0'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 5780\n",
      "Starting data generators\n",
      "Starting data generators\n",
      "Found 1746 validated image filenames belonging to 2 classes.\n",
      "Found 436 validated image filenames belonging to 2 classes.\n",
      "Found 242 validated image filenames belonging to 2 classes.\n",
      "TOTAL: 2424\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PROP = 0.8\n",
    "VALID_PROP = 0.1\n",
    "TEST_PROP = 0.1\n",
    "SEED = 42\n",
    "\n",
    "print(f'N: {len(in_data)}')\n",
    "\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 5\n",
    "BS = 32\n",
    "SHUFFLE = True\n",
    "DROPOUT = 0.5\n",
    "# EARLY_STOPPING = 10\n",
    "OPTIMIZER = 'Adam'\n",
    "DENSE_UNITS = 128\n",
    "\n",
    "print('Starting data generators')\n",
    "train_valid_df = final_df.sample(frac=TRAIN_PROP+VALID_PROP, random_state=SEED)\n",
    "test_df = final_df[~final_df.img_name.isin(train_valid_df.img_name)]\n",
    "# train_valid_df = in_data.sample(frac=TRAIN_PROP+VALID_PROP, random_state=SEED)\n",
    "# test_df = in_data[~in_data.img_name.isin(train_valid_df.img_name)]\n",
    "\n",
    "print('Starting data generators')\n",
    "datagen = ImageDataGenerator(preprocessing_function=prep_input_mobilenetv2, \n",
    "                             validation_split=0.2)\n",
    "\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(train_valid_df, \n",
    "                                        x_col=\"img_name\", \n",
    "                                        y_col=\"comp\",\n",
    "                                        target_size=(224, 224),\n",
    "                                        class_mode=\"binary\",\n",
    "                                        batch_size=BS, \n",
    "                                        subset='training')\n",
    "\n",
    "validation_gen = datagen.flow_from_dataframe(train_valid_df,\n",
    "                                            x_col=\"img_name\", \n",
    "                                            y_col=\"comp\",\n",
    "                                            target_size=(224, 224),\n",
    "                                            class_mode=\"binary\",\n",
    "                                            batch_size=BS, \n",
    "                                            subset='validation')\n",
    "\n",
    "test_gen = datagen.flow_from_dataframe(test_df,\n",
    "                                       x_col=\"img_name\", \n",
    "                                       y_col=\"comp\",\n",
    "                                       target_size=(224, 224),\n",
    "                                       class_mode=\"binary\",\n",
    "                                       batch_size=BS)\n",
    "\n",
    "print(f'TOTAL: {train_gen.n + validation_gen.n + test_gen.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "PARAMS = {'batch_size': BS,\n",
    "          'n_epochs': EPOCHS,\n",
    "          'shuffle': SHUFFLE,\n",
    "          'dense_units': DENSE_UNITS,\n",
    "          'learning_rate': INIT_LR,\n",
    "          'optimizer': OPTIMIZER,\n",
    "          'dropout': DROPOUT,\n",
    "#           'early_stopping': EARLY_STOPPING,\n",
    "          'train_prop': TRAIN_PROP,\n",
    "          'validation_prop': VALID_PROP,\n",
    "          'test_prop': TEST_PROP,\n",
    "          'n_train': train_gen.n,\n",
    "          'n_validation': validation_gen.n,\n",
    "          'n_test': test_gen.n,\n",
    "          'seed': SEED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment\n",
      "https://ui.neptune.ai/guilhermemg/icao-nets-training/e/IC-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(IC-20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating experiment')\n",
    "neptune.create_experiment(name='train_mobilenetv2',\n",
    "                          params=PARAMS,\n",
    "                          properties={\n",
    "                                      'dl_names': str([dl_name.value for dl_name in dl_names]),\n",
    "                                      'dl_aligned': True,\n",
    "                                      'icao_req': req.value,\n",
    "                                      'tagger_model': m.get_model_name().value},\n",
    "                          description='Changing learning rate scheduler function to improve results from experiment IC-19',\n",
    "                          tags=['mobilenetv2'],\n",
    "                          upload_source_files=['train_mobilenetv2.py']\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./tensorboard_out/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback): \n",
    "    def __init__(self, val_gen):\n",
    "        self.val_gen = val_gen\n",
    "        self.out_file_path = 'output/out.csv'\n",
    "    \n",
    "    def __clean_out_file(self):\n",
    "        if os.path.exists(self.out_file_path):\n",
    "            os.remove(self.out_file_path)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        \n",
    "        \n",
    "        if epoch == 0:\n",
    "            self.__clean_out_file()\n",
    "        \n",
    "        print(f'\\nEpoch: {epoch}\\n')   \n",
    "        with open(self.out_file_path,'a') as f:\n",
    "            predIxs = self.model.predict(self.val_gen, batch_size=BS)\n",
    "            Y = self.val_gen.labels\n",
    "            Y_hat = np.argmax(predIxs, axis=1)\n",
    "            for idx,(y,y_h) in enumerate(zip(Y,Y_hat)):\n",
    "                if epoch == 0 and idx == 0:\n",
    "                    f.writelines('epoch,idx,y,y_hat\\n')\n",
    "                f.writelines(f'{epoch},{idx},{y},{y_h}\\n')\n",
    "    \n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         print(f'\\nbatch: {batch}')\n",
    "#         print(f'\\nlogs: {logs}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Epoch 1/5\n",
      " 2/54 [>.............................] - ETA: 8s - loss: 0.8690 - accuracy: 0.4531WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0651s vs `on_train_batch_end` time: 0.2541s). Check your callbacks.\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.5292\n",
      "Epoch: 0\n",
      "\n",
      "54/54 [==============================] - 7s 132ms/step - loss: 0.7911 - accuracy: 0.5292 - val_loss: 0.7040 - val_accuracy: 0.5553\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.5140\n",
      "Epoch: 1\n",
      "\n",
      "54/54 [==============================] - 5s 84ms/step - loss: 0.7159 - accuracy: 0.5140 - val_loss: 0.6978 - val_accuracy: 0.5240\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.7026 - accuracy: 0.5233\n",
      "Epoch: 2\n",
      "\n",
      "54/54 [==============================] - 4s 82ms/step - loss: 0.7026 - accuracy: 0.5233 - val_loss: 0.6957 - val_accuracy: 0.5144\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.5216\n",
      "Epoch: 3\n",
      "\n",
      "54/54 [==============================] - 4s 81ms/step - loss: 0.6981 - accuracy: 0.5216 - val_loss: 0.6946 - val_accuracy: 0.5168\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.5298\n",
      "Epoch: 4\n",
      "\n",
      "54/54 [==============================] - 5s 84ms/step - loss: 0.6960 - accuracy: 0.5298 - val_loss: 0.6941 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "print('Training network')\n",
    "\n",
    "log_dir = \"tensorboard_out/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint_filepath = '/output/checkpoint_epoch_{}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_freq='epoch')\n",
    "\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224,224,3),\n",
    "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(PARAMS['dense_units'], activation=\"relu\")(headModel)\n",
    "headModel = Dropout(PARAMS['dropout'])(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False\n",
    "\n",
    "# compile our model\n",
    "# opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=PARAMS['optimizer'], metrics=[\"accuracy\"])\n",
    "\n",
    "# Log model summary\n",
    "# model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "\n",
    "# train the head of the network\n",
    "H = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_gen.n // BS,\n",
    "        validation_data=validation_gen,\n",
    "        validation_steps=validation_gen.n // BS,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "#             LambdaCallback(on_epoch_end = lambda epoch, logs: log_data(logs)),\n",
    "#             EarlyStopping(patience=PARAMS['early_stopping'], monitor='accuracy', restore_best_weights=True),\n",
    "              LearningRateScheduler(lr_scheduler),\n",
    "              MyCallback(validation_gen),\n",
    "              tensorboard_callback\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Trained Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Saving model')\n",
    "# Log model weights\n",
    "with tempfile.TemporaryDirectory(dir='.') as d:\n",
    "    prefix = os.path.join(d, 'model_weights')\n",
    "    model.save_weights(os.path.join(prefix, 'model'))\n",
    "    for item in os.listdir(prefix):\n",
    "        neptune.log_artifact(os.path.join(prefix, item), os.path.join('model_weights', item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Trained Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NON_COMP       0.19      0.52      0.28       114\n",
      "        COMP       0.79      0.46      0.58       464\n",
      "\n",
      "    accuracy                           0.47       578\n",
      "   macro avg       0.49      0.49      0.43       578\n",
      "weighted avg       0.67      0.47      0.52       578\n",
      "\n",
      "0.4688581314878893\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Trained Model\")\n",
    "\n",
    "predIdxs = model.predict(test_gen, batch_size=BS)\n",
    "y_hat = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "print(classification_report(test_gen.labels, y_hat, target_names=['NON_COMP','COMP']))  \n",
    "print(accuracy_score(test_gen.labels, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating model')\n",
    "eval_metrics = model.evaluate(test_gen, verbose=0)\n",
    "for j, metric in enumerate(eval_metrics):\n",
    "    neptune.log_metric('eval_' + model.metrics_names[j], metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing Experiment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing Neptune\n"
     ]
    }
   ],
   "source": [
    "print('Finishing Neptune')\n",
    "neptune.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "neptune": {
   "notebookId": "d6d95e8a-b251-40a1-bf9d-610ebc484f63"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
